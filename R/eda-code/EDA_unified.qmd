---
title: "Class Project"
author: Andrew J. Ruiz
date: "2024-03-13"
output: html_document
---

# load needed packages. make sure they are installed.
```{r}
library(here)        # for data loading/saving
library(dplyr)       # for data manipulation
library(skimr)       # for data summarization
library(ggplot2)     # for data visualization
library(lubridate)   # for date and time manipulation
library(forecast)    # for time series forecasting
library(tsibble)     # for time series data wrangling
library(feasts)      # for decomposing and visualizing time series
library(RColorBrewer) # for color palettes
library(ggcorrplot)  # for correlation plot
library(readxl)      # for reading Excel files
library(corrplot)    # for correlation plot
library(plotly)      # for interactive plots
library(tidyr)       # for data reshaping
library(reshape2)
```

# This file will contain the EDA code for the class project.

# Calculate Minimum Infection Rate (MIR) by trap type and by week
### The Minimum Infection Rate (MIR) is a common metric used to estimate the prevalence of mosquito-borne viruses in a population. It is calculated as the number of positive mosquito pools per 1,000 mosquitoes tested. The MIR can provide insights into the risk of virus transmission to humans and animals.
#### Mosquito traps are designed to attract different life stages and species of mosquitoes. For this project, the focus will be on CDC Light Traps baited with CO2. These traps are used to trap a wide range of species, including the main zoonotic vector of EEE, Culiseta melanura (MEL). 
```{r}
# Read the virus isolation dataset
pcr_data <- read_excel(here("data", "raw-data", "virus_iso.xlsx"))

# Add week number based on the collection data
# This code ensures that the ISO 8601 standard is applied to the week number calculation
# Week 1 of any year is the week that contains January 4th, or equivalently, it's the week that contains the first Thursday of January.
# Weeks start on Monday and end on Sunday.
# The last week of the year, week 52 or 53, is the one that contains December 28th.
# we will use week_num to join tables
# the format will be YYYY-WW. Week 8 of 2019 will look like: 2019-08
pcr_data <- pcr_data %>%
  mutate(
    # Directly format the collection date into "YYYY-WW"
    week_num = paste0(year(`Collection Date`), "-", 
                      sprintf("%02d", isoweek(`Collection Date`)))
  )

filtered_pcr_data <- pcr_data %>%
  #filter(`Trap Type` %in% trap_types_to_include) %>%
  mutate(
    Submitted_for_Testing = ifelse(`Submitted for Testing` == "Y", 1, 0),  # Convert to numeric
    Is_Positive = ifelse(Result == "Positive", 1, 0),  # Convert Positive results to numeric
    Pool_Size_Numeric = as.numeric(`Pool Size`)  # Ensure Pool Size is numeric for summing - Corrected
  )

#save as csv
write_csv(filtered_pcr_data, here("data", "processed-data", "filtered_pcr_data.csv"))

#calculate MIR for all sites, all traps by week
MIR_by_week <- filtered_pcr_data %>%
  group_by(week_num) %>%
  summarise(
    Pools_Tested = sum(Submitted_for_Testing, na.rm = TRUE),
    Positive_Pools = sum(Is_Positive, na.rm = TRUE),
    Total_Tested = sum(ifelse(Submitted_for_Testing == 1, Pool_Size_Numeric, 0), na.rm = TRUE),  # Sum Pool Size for submitted tests - Corrected
    .groups = 'drop'
  ) %>%
  filter(Pools_Tested > 0) %>%  # Filter out weeks where no pools were tested
  mutate(
    MIR = ifelse(Total_Tested > 0, (Positive_Pools / Total_Tested) * 1000, 0),  # Calculate MIR
    Week = as.numeric(substring(week_num, 6)), 
    Year = as.numeric(substring(week_num, 1, 4))  # Extract week number and year
  )

# Save as CSV
write_csv(MIR_by_week, here("data", "processed-data", "MIR_by_week.csv"))

# Save as RDS
saveRDS(MIR_by_week, here("data", "processed-data", "rds", "MIR_by_week.rds"))
str(MIR_by_week)

#CALCULATE MIR BY TRAP TYPE
# Define the trap types to include
trap_types_to_include <- c("500cc CO2 - CDC Miniature Light Trap", "Dry Ice CO2 - CDC Miniature Light Trap with CO2")

# Filter pcr_data for specific CO2 baited light traps
filtered_pcr_data_co2 <- pcr_data %>%
  filter(`Trap Type` %in% trap_types_to_include) %>%
  mutate(
    Submitted_for_Testing = ifelse(`Submitted for Testing` == "Y", 1, 0),  # Convert to numeric
    Is_Positive = ifelse(Result == "Positive", 1, 0),  # Convert Positive results to numeric
    Pool_Size_Numeric = as.numeric(`Pool Size`)  # Ensure Pool Size is numeric for summing - Corrected
  )

MIR_by_week_co2 <- filtered_pcr_data_co2 %>%
  group_by(week_num) %>%
  summarise(
    Pools_Tested = sum(Submitted_for_Testing, na.rm = TRUE),
    Positive_Pools = sum(Is_Positive, na.rm = TRUE),
    Total_Tested = sum(ifelse(Submitted_for_Testing == 1, Pool_Size_Numeric, 0), na.rm = TRUE),  # Sum Pool Size for submitted tests - Corrected
    .groups = 'drop'
  ) %>%
  filter(Pools_Tested > 0) %>%  # Filter out weeks where no pools were tested
  mutate(
    MIR = ifelse(Total_Tested > 0, (Positive_Pools / Total_Tested) * 1000, 0),  # Calculate MIR
    Week = as.numeric(substring(week_num, 6)), 
    Year = as.numeric(substring(week_num, 1, 4))  # Extract week number and year
  )

# Save the calculated MIR for CO2 baited traps by week
write_csv(MIR_by_week_co2, here("data", "processed-data", "MIR_by_week_co2.csv"))
saveRDS(MIR_by_week_co2, here("data", "processed-data", "rds", "MIR_by_week_co2.rds"))

# Calculate MIR for MEL only
# Filter pcr_data for MEL only
pcr_mel_co2 <- pcr_data %>%
  filter(`Trap Type` %in% trap_types_to_include) %>% # Filter for CO2 baited traps
  filter(`Species code` == "MEL") %>% # Filter for pool tests
  mutate(
    Submitted_for_Testing = ifelse(`Submitted for Testing` == "Y", 1, 0), # Convert to numeric
    Is_Positive = ifelse(Result == "Positive", 1, 0), # Ensure Is_Positive is numeric
    Pool_Size_Numeric = as.numeric(`Pool Size`)  # Ensure Pool Size is numeric for summing - Corrected
  )

mir_mel_week_co2 <- pcr_mel_co2 %>%
  group_by(week_num) %>%
  summarise(
    Pools_Tested = sum(Submitted_for_Testing, na.rm = TRUE),
    Positive_Pools = sum(Is_Positive, na.rm = TRUE),
    Total_Tested = sum(ifelse(Submitted_for_Testing == 1, Pool_Size_Numeric, 0), na.rm = TRUE),  # Sum Pool Size for submitted tests - Corrected
    .groups = 'drop'
  ) %>%
  filter(Pools_Tested > 0) %>%  # Filter out weeks where no pools were tested
  mutate(
    MIR = ifelse(Total_Tested > 0, (Positive_Pools / Total_Tested) * 1000, 0),  # Calculate MIR
    Week = as.numeric(substring(week_num, 6)), 
    Year = as.numeric(substring(week_num, 1, 4))  # Extract week number and year
  )

# Save the calculated MIR for MEL in CO2 baited traps by week
write_csv(mir_mel_week_co2, here("data", "processed-data", "mir_mel_week_co2.csv"))
saveRDS(mir_mel_week_co2, here("data", "processed-data", "rds", "mir_mel_week_co2.rds"))

# Calculate MIR for all other species
# filter to remove MEL
pcr_nomel_co2 <- pcr_data %>%
  filter(`Trap Type` %in% trap_types_to_include) %>% # Filter for CO2 baited traps
  filter(`Species code` != "MEL") %>% # Filter for pool tests
  mutate(
    Submitted_for_Testing = ifelse(`Submitted for Testing` == "Y", 1, 0), # Convert to numeric
    Is_Positive = ifelse(Result == "Positive", 1, 0), # Ensure Is_Positive is numeric
    Pool_Size_Numeric = as.numeric(`Pool Size`)  # Ensure Pool Size is numeric for summing - Corrected
  )

mir_nomel_week_co2 <- pcr_nomel_co2 %>%
  group_by(week_num) %>%
  summarise(
    Pools_Tested = sum(Submitted_for_Testing, na.rm = TRUE),
    Positive_Pools = sum(Is_Positive, na.rm = TRUE),
    Total_Tested = sum(ifelse(Submitted_for_Testing == 1, Pool_Size_Numeric, 0), na.rm = TRUE),  # Sum Pool Size for submitted tests - Corrected
    .groups = 'drop'
  ) %>%
  filter(Pools_Tested > 0) %>%  # Filter out weeks where no pools were tested
  mutate(
    MIR = ifelse(Total_Tested > 0, (Positive_Pools / Total_Tested) * 1000, 0),  # Calculate MIR
    Week = as.numeric(substring(week_num, 6)), 
    Year = as.numeric(substring(week_num, 1, 4))  # Extract week number and year
  )

# Save the calculated MIR for all other species in CO2 baited traps by week
write_csv(mir_nomel_week_co2, here("data", "processed-data", "mir_nomel_week_co2.csv"))
saveRDS(mir_nomel_week_co2, here("data", "processed-data", "rds", "mir_nomel_week_co2.rds"))

# Finally, we will calculate MIR by year.
# this will be used for a second model
mir_year <- filtered_pcr_data_co2 %>%
  mutate(
    Year = as.numeric(substring(week_num, 1, 4))  # Extract the year
  ) %>%
  group_by(Year) %>%
  summarise(
    Pools_Tested = sum(Submitted_for_Testing, na.rm = TRUE),
    Positive_Pools = sum(Is_Positive, na.rm = TRUE),
    Total_Tested = sum(ifelse(Submitted_for_Testing == 1, Pool_Size_Numeric, 0), na.rm = TRUE),  # Correctly sum Pool Size for submitted tests
    .groups = 'drop'
  ) %>%
  filter(Pools_Tested > 0) %>%  # Filter out years where Pools_Tested is 0
  mutate(
    MIR = ifelse(Total_Tested > 0, (Positive_Pools / Total_Tested) * 1000, 0)  # Calculate MIR
  )
filtered_pcr_data_co2
mir_year
```

# Join the MIR data with ither datasets based on week number
#### First we will process some data that will be used in the join
```{r}
# Load the datasets:
MIR_by_week_co2 <- read_csv(here("data", "processed-data", "MIR_by_week_co2.csv"))
wx_selected <- read_csv(here("data", "processed-data", "wx_selected.csv"))
mosquito_wide <- read_csv(here("data", "processed-data", "mosquito_wide_all.csv"))
drought_fema1_4 <- read_csv(here("data", "processed-data", "drought_fema1_4.csv"))
drought_ma_counties <- read_csv(here("data", "processed-data", "drought_ma_counties.csv"))
average_daylight_by_week <- read_csv(here("data", "processed-data", "average_daylight_by_week.csv"))
average_mel_weekco2 <- read_csv(here("data", "processed-data", "average_mel_week_co2.csv"))
wx_hourly_clean <- read_csv(here("data", "processed-data", "hourly_wx_clean.csv"))
mel_prop_weekly <- read_csv(here("data", "processed-data", "mel_proportion_by_week.csv"))

# Before joining, there are some modifications to make:
# Aggregate weather data by week:
# Calculate the average maximum temperature (Avg_TMAX) and total precipitation (Total_PRCP) for each week.
# This simplification allows for weekly comparisons with the MIR data.
wx_aggregated <- wx_selected %>%
  group_by(week_num) %>%
  summarise(Avg_TMAX = mean(TMAX, na.rm = TRUE),
            Total_PRCP = sum(PRCP, na.rm = TRUE),
            .groups = 'drop')

# Adjust for a one-week lag in weather data:
# Create a new column (Lead_week_num) that represents each week's preceding week. 
# This adjustment accounts for the delayed effect weather might have on MIR.
wx_aggregated_lead <- wx_aggregated %>%
  mutate(
    # Split week_num to get year and week as numeric
    year = as.integer(substr(week_num, 1, 4)),
    week = as.integer(substr(week_num, 6, 8)),
    # Calculate the next week, accounting for year transition
    Lead_week_num = pmap_chr(list(year, week), function(year, week) {
      last_week_of_year = ifelse(lubridate::isoweek(lubridate::ymd(paste(year, "12", "28"))) == 53, 53, 52)
      if (week == last_week_of_year) {
        # Transition to the first week of the next year
        lead_year = year + 1
        paste0(lead_year, "-", sprintf("%02d", 1))
      } else {
        # Just add one to the week for the same year
        paste0(year, "-", sprintf("%02d", week + 1))
      }
    })
  ) %>%
  select(-year, -week) # Removing intermediate columns

str(wx_aggregated_lead)

# Calculate a new colum in hourly weather data for number of hours the dry bulb temperature is above 75 and 80 degrees
# Calculating the conditions and averaging other columns
wx_hourly <- wx_hourly_clean %>%
  group_by(week_num) %>%
  summarise(
    Hrs_Above_80_lag = sum(HourlyDryBulbTemperature >= 80, na.rm = TRUE),
    Hrs_Above_75_lag = sum(HourlyDryBulbTemperature >= 75, na.rm = TRUE),
    Hrs_Below_50_lag = sum(HourlyDryBulbTemperature < 50, na.rm = TRUE),
    Hrs_Between_50_86_lag = sum(HourlyDryBulbTemperature >= 50 & HourlyDryBulbTemperature <= 86, na.rm = TRUE), # Added line for temperature between 50 and 86
    Avg_HourlyDryBulbTemp_lag = mean(HourlyDryBulbTemperature, na.rm = TRUE),
    Avg_HourlyWetBulbTemp_lag = mean(HourlyWetBulbTemperature, na.rm = TRUE),
    .groups = 'drop'
  )
wx_hourly_lead <- wx_hourly %>%
  mutate(
    Year = as.integer(substr(week_num, 1, 4)),
    Week = as.integer(substr(week_num, 6, 7)),
    Lead_week_num = map2_chr(Year, Week, ~ {
      last_week_of_year = ifelse(isoweek(ymd(paste(.x, "12", "28"))) == 53, 53, 52)
      if (.y == last_week_of_year) {
        lead_year = .x + 1
        paste0(lead_year, "-", sprintf("%02d", 1))
      } else {
        paste0(.x, "-", sprintf("%02d", .y + 1))
      }
    })
  )

# Define the path for the updated CSV file
updated_csv_path <- here("data", "processed-data", "wx_hourly_lead.csv")

# Save the updated dataframe
write.csv(wx_hourly_lead, updated_csv_path, row.names = FALSE)

# There are some other predictors from the same hourly dataset that are likely to affect the MIR that do not need a lag
# Calculate the # of hours below 50, average relative humidity, wet bulb temperature, and wind speed for each week
wx_hourly_current <- wx_hourly_clean %>%
  group_by(week_num) %>%
  summarise(
    Hrs_Below_50_current = sum(HourlyDryBulbTemperature < 50, na.rm = TRUE),
    Avg_HourlyRelativeHumidity_current = mean(HourlyRelativeHumidity, na.rm = TRUE),
    Avg_HourlyWetBulbTemperature_current = mean(HourlyWetBulbTemperature, na.rm = TRUE),
    Avg_HourlyWindSpeed_current = mean(HourlyWindSpeed, na.rm = TRUE),
    .groups = 'drop'
  )

#save as csv
write_csv(wx_hourly_current, here("data", "processed-data", "wx_hourly_current.csv"))

```

#### Now we will join the MIR data with the other datasets based on the week number
```{r}
combined_data_mir_all <- MIR_by_week_co2 %>%
  # Joining wx_aggregated and wx_hourly_summary on a lagged week_num
  left_join(wx_aggregated_lead, by = c("week_num" = "Lead_week_num")) %>%
  left_join(wx_hourly_lead, by = c("week_num" = "Lead_week_num")) %>%
  # Direct joins for other datasets
  left_join(wx_hourly_current %>% distinct(week_num, .keep_all = TRUE), by = "week_num") %>%
  left_join(drought_fema1_4 %>% distinct(week_num, .keep_all = TRUE), by = "week_num") %>%
  left_join(drought_ma_counties %>% distinct(week_num, .keep_all = TRUE), by = "week_num") %>%
  left_join(average_daylight_by_week %>% distinct(week_num, .keep_all = TRUE), by = "week_num") %>%
  left_join(average_mel_weekco2 %>% distinct(week_num, .keep_all = TRUE), by = "week_num") %>%
  left_join(mel_prop_weekly %>% distinct(week_num, .keep_all = TRUE), by = "week_num")


# Verify the structure and first few rows to ensure accuracy
head(combined_data_mir_all)


# Further cleaning and processing of the combined data
# Remove unnecessary columns and keep only the first value for each week
combined_data_mir_all_clean <- combined_data_mir_all %>%
  ungroup() %>% # Ensure data is not grouped before proceeding
  dplyr::select(
    -Year.y, -MapDate.x, -ValidEnd.x, -MapDate.y, -State, 
    -StatisticFormatID.y, -week_num.y, -Week.y, -RegionName, -Year.x,
    -StatisticFormatID.x, -FIPS, -Week.x, -ValidStart.x, -ValidStart.y, -County, -ValidEnd.y
  ) #%>%
  #summarise(across(everything(), first), .groups = 'drop')
str(combined_data_mir_all_clean)
# Save the modified dataframe as a CSV
write_csv(combined_data_mir_all_clean, here("data", "processed-data", "joined_data_mir_co2.csv"))

```
### In addition to the MIR data for CO2 traps only, we will also create a dataset for all trap types. 
```{r}
all_MIR <- read_csv(here("data", "processed-data", "MIR_by_week.csv"))

#join with other datasets
all_mir <- all_MIR %>%
  # Joining wx_aggregated and wx_hourly_summary on a lagged week_num
  left_join(wx_aggregated_lead, by = c("week_num" = "Lead_week_num")) %>%
  left_join(wx_hourly_lead, by = c("week_num" = "Lead_week_num")) %>%
  # Direct joins for other datasets
  left_join(wx_hourly_current %>% distinct(week_num, .keep_all = TRUE), by = "week_num") %>%
  left_join(drought_fema1_4 %>% distinct(week_num, .keep_all = TRUE), by = "week_num") %>%
  left_join(drought_ma_counties %>% distinct(week_num, .keep_all = TRUE), by = "week_num") %>%
  left_join(average_daylight_by_week %>% distinct(week_num, .keep_all = TRUE), by = "week_num") %>%
  left_join(average_mel_weekco2 %>% distinct(week_num, .keep_all = TRUE), by = "week_num") %>%
  left_join(mel_prop_weekly %>% distinct(week_num, .keep_all = TRUE), by = "week_num")

# Verify the structure and first few rows to ensure accuracy
head(all_mir)

# Now we will clean the data as above
all_mir_clean <- all_mir %>%
  ungroup() %>% # Ensure data is not grouped before proceeding
  dplyr::select(
    -Year.y, -MapDate.x, -ValidEnd.x, -MapDate.y, -State, 
    -StatisticFormatID.y, -week_num.y, -Week.y, -RegionName, -Year.x,
    -StatisticFormatID.x, -FIPS, -Week.x, -ValidStart.x, -ValidStart.y, -County, -ValidEnd.y
  ) #%>%
  #summarise(across(everything(), first), .groups = 'drop')
str(all_mir_clean)
# Save the modified dataframe as a CSSV
write_csv(all_mir_clean, here("data", "processed-data", "all_mir_clean_joined.csv"))

```

## This is a scatter plot of the average number of MEL mosquitoes that were captured in light traps and the mosquito infection rate (MIR).
```{r}
joined_mir <- read_csv(here("data", "processed-data", "joined_data_mir_co2.csv"))

# code to create a function for scatter plot
create_scatter_plot <- function(data, x_var, y_var) {
  # Ensure the variables exist in the dataframe
  if (!(x_var %in% names(data)) || !(y_var %in% names(data))) {
    stop("One or both of the specified variables do not exist in the dataframe.")
  }
  
  # Generate the scatter plot using the .data pronoun for NSE
  p <- ggplot(data, aes(x = .data[[x_var]], y = .data[[y_var]])) +
    geom_point() + # Plot points
    geom_smooth(method = "loess", color = "blue") + # Add a LOESS smoothed curve
    theme_minimal() +
    labs(x = x_var, y = y_var, title = paste("Scatter Plot of", x_var, "vs.", y_var))
  
  print(p)
}

# Example usage with your dataframe
create_scatter_plot(joined_mir, "Hrs_Above_80_lag", "MIR")

# DSCI_ma is an indicators of drought severity. The higher the number the more severe and widespread the drought.
create_scatter_plot(joined_mir, "DSCI_ma", "MIR")

# Propotion of MEL mosquitoes captured vs MIR
create_scatter_plot(joined_mir, "MEL_Prop", "MIR")

# This is the average high temperature for the week before the mosquito was collected
create_scatter_plot(joined_mir, "Avg_TMAX", "MIR")

# This is the total precipitation for the week before the mosquito was collected
create_scatter_plot(joined_mir, "Total_PRCP", "MIR")

# This variable is the number of hours the temperature is between 50 and 86 degrees. Below 50, mosquitoes are less active. The effect of temperature above 86 on mosquitoes does not increase developemt speed. 
create_scatter_plot(joined_mir, "Hrs_Between_50_86_lag", "MIR")

# This is the average relative humidity for the week the mosquito was collected
create_scatter_plot(joined_mir, "Avg_HourlyRelativeHumidity_current", "MIR")

# This is the average wind speed for the week the mosquito was collected
create_scatter_plot(joined_mir, "Avg_HourlyWindSpeed_current", "MIR")

# This is the average dry bulb temperature for the week before the mosquito was collected
create_scatter_plot(joined_mir, "Avg_HourlyDryBulbTemp_lag", "MIR")

# this is the total # of mosquitoes tested for the week
# this should not be a predictor but it is interesting to see if there is a relationship
# it might be better to calculate the average count per trap
create_scatter_plot(joined_mir, "Total_Tested", "MIR")
```

#Distribution of MIR
### Before deciciding on models, we should check how the outcome data is distributed.
#### Load MIR data
```{r}
# Load the dataset:
joined_mir <- read_csv(here("data", "processed-data", "joined_data_mir_co2.csv"))
```
#### Remove rows with missing cells 
```{r}
# # check for missing MIR values
# na_rows_mir = joined_mir[is.na(joined_mir$MIR), ]
# head(na_rows_mir)  # To view the first few rows
# 
# # Remove rows with missing MIR values
# joined_mir <- joined_mir[!is.na(joined_mir$MIR), ]
# 
# 
# #save the cleaned data
# write_csv(joined_mir, here("data", "processed-data", "joined_data_mir_co2_cleaned.csv"))
```

### Visualize the distribution of MIR
```{r}
p <- ggplot(joined_mir, aes(x = MIR)) +
  geom_density(fill = "tomato", alpha = 0.7) +  # Adjust color and transparency
  theme_minimal() +  # Cleaner theme
  labs(title = "Density Plot of MIR",
       x = "MIR",
       y = "Density") +
  theme(plot.title = element_text(hjust = 0.5),  # Center the title
        text = element_text(size = 12))  # Adjust text size

# Display the plot
print(p)

#save the plot
ggsave(here("results", "figures", "mir_density_plot.png"), plot = p, width = 8, height = 6, units = "in")

```
#### Based on the histogram and density plot, the outcome of interest (MIR) appears to be right-skewed and not normally distributed. This may impact the choice of modeling techniques and the interpretation of results.

### Check for excess zeros and over dispersion
```{r}

# Check for excess zeros
zero_proportion <- sum(joined_mir$MIR == 0) / nrow(joined_mir)
paste("Proportion of zeros:", round(zero_proportion, 4))

# Calculate the mean and variance of MIR
mir_mean <- mean(joined_mir$MIR, na.rm = TRUE)
mir_variance <- var(joined_mir$MIR, na.rm = TRUE)

# Now print the mean and variance
cat("Mean of MIR:", mir_mean, "\n")
cat("Variance of MIR:", mir_variance, "\n")

# Calculate the variance-to-mean ratio (VMR)
vmr <- mir_variance / mir_mean
cat("Variance-to-Mean Ratio (VMR) of MIR:", vmr, "\n")

# Check for overdispersion
if(vmr > 1) {
  cat("Evidence of overdispersion: VMR is greater than 1.\n")
} else {
  cat("No evidence of overdispersion: VMR is not greater than 1.\n")
}
```
#### Given the excess zeros and the overdispersion in the MIR data, it may be appropriate to consider a Generalized Additive Model (GAM). GAM allow for 2-part models that can models 
# Predictors
#### Closer look at the possible predictors for the model
```{r}
# Load the dataset:
# combined_data_mir_all: Contains weekly MIR data and possible predictors for the model.
joined_mir <- read_csv(here("data", "processed-data", "joined_data_mir_co2.csv"))
joined_mir
# Assuming 'joined_mir' is your dataframe
continuous_vars <- joined_mir[, c("Avg_TMAX", "Total_PRCP", "Hrs_Above_80_lag", 
                                  "Hrs_Above_75_lag", "Hrs_Below_50_lag", 
                                  "Avg_HourlyDryBulbTemp_lag", "Avg_HourlyRelativeHumidity_current", 
                                  "Avg_HourlyWetBulbTemperature_current", "Avg_HourlyWindSpeed_current", 
                                  "DSCI_fema", "average_daylight_hours", 
                                  "Average_MEL", "Total_MEL", "MEL_Prop")]

# Summarize missing values for each variable
missing_values <- sapply(joined_mir[, c("Avg_TMAX", "Total_PRCP", "Hrs_Above_80_lag", 
                                        "Hrs_Above_75_lag", "Hrs_Below_50_lag", 
                                        "Avg_HourlyDryBulbTemp_lag", "Avg_HourlyRelativeHumidity_current", 
                                        "Avg_HourlyWetBulbTemperature_current", "Avg_HourlyWindSpeed_current", 
                                        "DSCI_fema", "average_daylight_hours", 
                                        "Average_MEL", "Total_MEL", "MEL_Prop")], function(x) sum(is.na(x)))
print(missing_values)


# Create a correlation matrix
cor_matrix <- cor(continuous_vars, use = "complete.obs")  # 'use' parameter handles missing values

#cor_matrix[abs(cor_matrix) < 0.5] <- NA

# Ensure cor_matrix is a correct correlation matrix
if (!is.matrix(cor_matrix)) {
  cor_matrix <- as.matrix(cor_matrix)
}

# Melt the correlation matrix into a long format
cor_data <- melt(cor_matrix, na.rm = TRUE)

# Check that Var1 and Var2 exist
if (!all(c("Var1", "Var2") %in% names(cor_data))) {
  stop("Var1 and Var2 not found in melted data. Check the melting process.")
}

# Create the interactive heatmap
plot_ly(
  data = cor_data,
  x = ~Var1,
  y = ~Var2,
  z = ~value,
  type = 'heatmap',
  colorscale = 'Portland'
) %>%
  layout(
    title = 'Correlation Matrix',
    xaxis = list(title = 'Variable 1', tickangle = 45),
    yaxis = list(title = 'Variable 2', autorange = "reversed")
  )

```
