---
title: "statistical-analysis"
author: "Andrew Ruiz"
format: html
editor: source
---

# Load the necessary libraries for the analysis
```{r}
library(here)
library(dplyr)         # for data manipulation tasks like filtering, mutating, summarizing, and arranging
library(lubridate)     # for working with dates and times, offering functions to parse, manipulate, and format date-time objects
library(readr)         # for reading and writing rectangular data in text format, providing faster and more consistent reading of delimited files
library(stargazer)     # for creating LaTeX and ASCII tables for descriptive statistics and regression models
library(mgcv)          # for generalized additive models (GAMs) and other generalized ridge regression models
library(tidyr)         # for tidying and reshaping data for analysis and visualization
library(caret)         # for machine learning tasks like model training, tuning, and evaluation
library(randomForest)  # for building random forest models, a popular machine learning algorithm
library(tidymodels)    # for modeling and machine learning tasks using tidy principles
library(ranger)        # for fast, robust, and scalable random forest algorithms
library(dials)         # for tuning hyperparameters in models
library(vip)           # for variable importance assessment in predictive models
library(ggplot2)       # for creating elegant and customizable visualizations with the grammar of graphics
library(boot)          # for bootstrap resampling and estimating standard errors and confidence intervals
library(yardstick)     # for evaluating predictive models using performance metrics
library(glmnet)        # for fitting generalized linear models with LASSO or ridge regularization
library(mlr3)          # for building, evaluating, and comparing machine learning models
library(data.table)    # for efficient data manipulation and aggregation operations
library(Matrix)        # for sparse and dense matrix operations
library(gbm)           # for gradient boosting machines, a powerful machine learning algorithm
library(knitr)         # for dynamic report generation, integrating R code and output into documents
library(gt)            # for creating beautiful and customizable tables in R Markdown and other formats
library(xgboost)       # for extreme gradient boosting, a scalable and accurate machine learning algorithm)

```

## First logistic model

#### Effect of temperature and precipitation the week prior to mosquito collection on the likelihood of observing at least one positive mosquito test result in a week

```{r}
# Load the PCR data set which includes information on mosquito pools tested for viruses, 
# results of those tests, and the date of collection.
pcr <- read_csv(here("data", "processed-data", "filtered_pcr_data.csv"))

# Load the weather data set which includes daily maximum temperatures (TMAX) and other weather variables.
wx_selected <- read_csv(here("data", "processed-data", "wx_selected.csv"))

# Aggregate the weather data by week to calculate the average maximum temperature for each week.
# This simplifies the daily temperature data into a more manageable weekly summary,
# making it easier to correlate with weekly mosquito test results.
wx_selected <- wx_selected %>%
  group_by(week_num) %>%
  summarise(
    Avg_TMAX = mean(TMAX, na.rm = TRUE),
    Total_PRCP = sum(PRCP, na.rm = TRUE)
  ) %>%
  ungroup()

# View the first few rows to confirm it worked
head(wx_selected)


# Create a lagged version of the average weekly maximum temperature to explore the potential delayed effect of temperature on mosquito test results.
# The lag function shifts the temperature data by one week, assuming that the impact of temperature on virus transmission might not be immediate.
weather_weekly <- wx_selected %>%
  arrange(week_num) %>%
  mutate(Lagged_Avg_TMAX = lag(Avg_TMAX, 1))

weather_weekly <- wx_selected %>%
  arrange(week_num) %>%
  mutate(
    Lagged_Avg_TMAX = lag(Avg_TMAX, 1),
    Lagged_Total_PRCP = lag(Total_PRCP, 1)
  )

weather_weekly <- wx_selected %>%
  arrange(week_num) %>%
  mutate(
    Lagged_Avg_TMAX = lag(Avg_TMAX, 1),
    Lagged_Total_PRCP = lag(Total_PRCP, 1),
    Binary_PRCP = if_else(Lagged_Total_PRCP > 0, 1, 0)
  )
# Aggregate the PCR data by week to determine if there was at least one positive result in each week.
# This transformation reduces the detailed test results into a weekly summary,
# indicating the presence or absence of positive tests each week.
pcr_weekly <- pcr %>%
  group_by(week_num) %>%
  summarise(Positive_Result = ifelse(all(is.na(Is_Positive)), 0, max(as.integer(Is_Positive), na.rm = TRUE)))

# Merge the aggregated PCR data with the lagged weather data by matching their week numbers.
# This combined data set enables the analysis of the relationship between weekly weather conditions and mosquito test results.
combined_wx_pcr <- merge(pcr_weekly, weather_weekly, by = "week_num")

# Fit a logistic regression model to examine the influence of lagged average weekly maximum temperature on the likelihood of observing at least one positive mosquito test result in a week.
# The binomial family specifies that the dependent variable (Positive_Result) is binary,
# making logistic regression an appropriate modeling choice for this binary outcome.
logistic_model_temp <- glm(Positive_Result ~ Lagged_Avg_TMAX + Binary_PRCP, family = binomial, data = combined_wx_pcr)

# Output the summary of the logistic regression model to review the estimated coefficients, their statistical significance, and the overall model fit.
# This summary provides insights into the relationship between temperature and the probability of positive mosquito test results.
summary(logistic_model_temp)

# Output a LaTeX-formatted table to the console
stargazer(logistic_model_temp, type = "text")

par(mfrow = c(2, 2))
plot(logistic_model_temp)

```

#### This script starts by loading essential R libraries necessary for data manipulation, reading, and date handling, ensuring a smooth setup for data analysis. It then loads PCR test results and weather data, providing foundational information on mosquito test results and environmental conditions to study patterns of disease spread and mosquito activity. Finally, it calculates average maximum temperatures for key months before the mosquito breeding season, linking climatic conditions to mosquito behavior and potential disease transmission risks.
```{r}
# Step 1: load the data
pcr <- read_csv(here("data", "processed-data", "pcr_data.csv"))
wx_selected <- read_csv(here("data", "processed-data", "wx_selected.csv"))

# Step 2: Calculate the average TMAX for April, May, and June separately
avg_temp_by_month <- wx_selected %>%
  filter(month(DATE) %in% c(4, 5, 6)) %>%
  group_by(Year = year(DATE), Month = month(DATE)) %>%
  summarise(Avg_TMAX = mean(TMAX, na.rm = TRUE), .groups = 'drop')

# Pivot the average temperatures into separate columns for each month
avg_temp_by_month_wide <- avg_temp_by_month %>%
  pivot_wider(names_from = Month, values_from = Avg_TMAX, names_prefix = "Avg_TMAX_M") %>%
  # Ensure all years are present even if some data might be missing
  complete(Year, fill = list(Avg_TMAX_M4 = NA, Avg_TMAX_M5 = NA, Avg_TMAX_M6 = NA))

# Step 4: Identify the first positive test date each year in mosquito_data
first_positive_dates <- pcr %>%
  filter(Result == "Positive") %>%
  group_by(Year = year(`Collection Date`)) %>%
  summarise(First_Positive_Date = min(`Collection Date`), .groups = 'drop')

# Step 5: Merge the datasets on Year
combined_data_firstpos <- left_join(first_positive_dates, avg_temp_by_month_wide, by = "Year")

# Step 6: Convert First_Positive_Date to a numerical value (e.g., day of the year)
combined_data_firstpos <- combined_data_firstpos %>%
  mutate(First_Positive_DOY = yday(First_Positive_Date))

# Explore the relationship via linear regression with Avg_TMAX_M4, Avg_TMAX_M5, Avg_TMAX_M6 as predictors
model2 <- lm(First_Positive_DOY ~ Avg_TMAX_M4 + Avg_TMAX_M5 + Avg_TMAX_M6, data = combined_data_firstpos)
summary(model2)

par(mfrow = c(2, 2))
plot(model2)

```

# Does the total precipitation in April, May, and June influence the date of the first positive test?

```{r}
wx_selected <- read_csv(here("data", "processed-data", "wx_selected.csv"))

# Calculate average TMAX and total PRCP for April, May, and June separately
avg_temp_and_prcp <- wx_selected %>%
  filter(month(DATE) %in% c(4, 5, 6)) %>%
  group_by(Year = year(DATE), Month = month(DATE)) %>%
  summarise(
    Avg_TMAX = mean(TMAX, na.rm = TRUE),
    Total_PRCP = sum(PRCP, na.rm = TRUE),
    .groups = 'drop'
  )

# Continue from the summarise step
wide_data <- avg_temp_and_prcp %>%
  # Convert to a long format to distinguish between Avg_TMAX and Total_PRCP
  pivot_longer(cols = c(Avg_TMAX, Total_PRCP), names_to = "Measure", values_to = "Value") %>%
  # Create a combined name for later pivot_wider
  unite("NewName", Measure, Month, sep = "_M") %>%
  # Pivot wider with a single names_prefix
  pivot_wider(names_from = NewName, values_from = Value) %>%
  complete(Year, fill = list(Avg_TMAX_M4 = NA, Avg_TMAX_M5 = NA, Avg_TMAX_M6 = NA, Total_PRCP_M4 = NA, Total_PRCP_M5 = NA, Total_PRCP_M6 = NA))

# Load and prepare mosquito data
first_positive_dates <- pcr %>%
  filter(Result == "Positive") %>%
  group_by(Year = year(`Collection Date`)) %>%
  summarise(First_Positive_Date = min(`Collection Date`), .groups = 'drop')

# Merge the datasets on Year
combined_data <- left_join(first_positive_dates, wide_data, by = "Year")

# Convert First_Positive_Date to a numerical value (e.g., day of the year)
combined_data <- combined_data %>%
  mutate(First_Positive_DOY = yday(First_Positive_Date))

# Explore the relationship via linear regression
model <- lm(First_Positive_DOY ~ Total_PRCP_M4 + Total_PRCP_M5 + Total_PRCP_M6, data = combined_data)
summary(model)

```

## Two-stage modeling approach using Generalized Additive Models (GAMs) to understand the factors influencing Mosquito Infection Rate (MIR).

### First stage: zero inflation model

#### This model predicts whether MIR is greater than zero using smoothed functions of several predictors, such as the Drought Severity Composite Index for the area (DSCI_ma), average mosquito larvae counts (Average_MEL), and the average current hourly wind speed (Avg_HourlyWindSpeed_current).

##### *The package required for this is mgcv*

```{r}
# load the dataset with a unique dataframe name
mir_2part <- read_csv(here("data", "processed-data", "joined_data_mir_co2.csv"))

#define the model
zero_gam_model <- gam(I(MIR > 0) ~ s(DSCI_ma) + s(Average_MEL) + s(Avg_HourlyWindSpeed_current),
                      family = binomial(link = "logit"), 
                      data = mir_2part)

summary(zero_gam_model)
plot(zero_gam_model)

# Define the null model with only an intercept
null_gam_model <- gam(I(MIR > 0) ~ 1,
                      family = binomial(link = "logit"), 
                      data = mir_2part)

summary(null_gam_model)

# Calculate the AIC for the null model
aic_null_gam_model <- AIC(null_gam_model)
aic_null_gam_model



aic_zero_gam_model <- AIC(zero_gam_model)
aic_zero_gam_model

# load the dataset with a unique dataframe name
mir_2part_all <- read_csv(here("data", "processed-data", "all_mir_clean_joined.csv"))

#define the model
zero_gam_model <- gam(I(MIR > 0) ~ s(DSCI_ma) + s(Average_MEL) + s(Avg_HourlyWindSpeed_current),
                      family = binomial(link = "logit"), 
                      data = mir_2part)

summary(zero_gam_model)
plot(zero_gam_model)

# Define the null model with only an intercept
null_gam_model <- gam(I(MIR > 0) ~ 1,
                      family = binomial(link = "logit"), 
                      data = mir_2part)

summary(null_gam_model)

# Calculate the AIC for the null model
aic_null_gam_model <- AIC(null_gam_model)
aic_null_gam_model



aic_zero_gam_model <- AIC(zero_gam_model)
aic_zero_gam_model
```

```{r}

# Response variable
response_var <- "MIR"

# Specified predictors for modeling
specified_predictors <- c("Avg_TMAX", "Total_PRCP", "Hrs_Above_80_lag", "Hrs_Above_75_lag",
                          "Hrs_Below_50_lag", "Hrs_Between_50_86_lag", "Avg_HourlyDryBulbTemp_lag",
                          "Avg_HourlyWetBulbTemp_lag", "Hrs_Below_50_current", "DSCI_fema",
                          "Avg_HourlyRelativeHumidity_current", "Avg_HourlyWetBulbTemperature_current", 
                          "Avg_HourlyWindSpeed_current", "None.x", "DSCI_fema", "average_daylight_hours", "Average_MEL", "MEL_Prop")

# Initiate lists to store models and summaries
gam_models_zero <- list()
gam_models_positive <- list()
summary_models_zero <- list()
summary_models_positive <- list()

# Loop through each specified predictor
for (predictor in specified_predictors) {
  # Check if predictor exists in the dataset
  if (!predictor %in% names(mir_2part)) {
    cat("Skipping", predictor, "because it is not in the dataset.\n")
    next
  }

  # Zero-inflation part
  formula_zero <- as.formula(paste0("I(", response_var, " > 0) ~ s(", predictor, ")"))
  zero_model <- gam(formula_zero, data = mir_2part, family = binomial(link = "logit"))
  gam_models_zero[[predictor]] <- zero_model
  summary_models_zero[[predictor]] <- summary(zero_model)

  cat("Zero-inflation model for", predictor, "\n")

  # Positive counts part
  if (any(mir_2part[[response_var]] > 0)) {
    positive_data <- mir_2part[mir_2part[[response_var]] > 0, ]
    formula_positive <- as.formula(paste0(response_var, " ~ s(", predictor, ")"))
    positive_model <- gam(formula_positive, data = positive_data, family = gaussian())
    gam_models_positive[[predictor]] <- positive_model
    summary_models_positive[[predictor]] <- summary(positive_model)

    cat("Positive model for", predictor, "\n")
  }
  
  # print or plot the summaries here...
  print(summary(zero_model))
  print(summary(positive_model))
  plot(zero_model)
  plot(positive_model)
}


### ALL MIR###
# Response variable
response_var <- "MIR"

# Specified predictors for modeling
specified_predictors <- c("Avg_TMAX", "Total_PRCP", "Hrs_Above_80_lag", "Hrs_Above_75_lag",
                          "Hrs_Below_50_lag", "Hrs_Between_50_86_lag", "Avg_HourlyDryBulbTemp_lag",
                          "Avg_HourlyWetBulbTemp_lag", "Hrs_Below_50_current", "DSCI_fema",
                          "Avg_HourlyRelativeHumidity_current", "Avg_HourlyWetBulbTemperature_current", 
                          "Avg_HourlyWindSpeed_current", "None.x", "DSCI_fema", "average_daylight_hours", "Average_MEL", "MEL_Prop")

# Initiate lists to store models and summaries
gam_models_zero <- list()
gam_models_positive <- list()
summary_models_zero <- list()
summary_models_positive <- list()

# Loop through each specified predictor
for (predictor in specified_predictors) {
  # Check if predictor exists in the dataset
  if (!predictor %in% names(mir_2part_all)) {
    cat("Skipping", predictor, "because it is not in the dataset.\n")
    next
  }

  # Zero-inflation part
  formula_zero <- as.formula(paste0("I(", response_var, " > 0) ~ s(", predictor, ")"))
  zero_model <- gam(formula_zero, data = mir_2part_all, family = binomial(link = "logit"))
  gam_models_zero[[predictor]] <- zero_model
  summary_models_zero[[predictor]] <- summary(zero_model)

  cat("Zero-inflation model for", predictor, "\n")

  # Positive counts part
  if (any(mir_2part_all[[response_var]] > 0)) {
    positive_data <- mir_2part_all[mir_2part_all[[response_var]] > 0, ]
    formula_positive <- as.formula(paste0(response_var, " ~ s(", predictor, ")"))
    positive_model <- gam(formula_positive, data = positive_data, family = gaussian())
    gam_models_positive[[predictor]] <- positive_model
    summary_models_positive[[predictor]] <- summary(positive_model)

    cat("Positive model for", predictor, "\n")
  }
  
  # print or plot the summaries here...
  print(summary(zero_model))
  print(summary(positive_model))
  plot(zero_model)
  plot(positive_model)
}
```

#### Based on the results above, the following have been identified as potential predictors of MIR:

##### This shows the model and the p-value
###### MIR ~ s(Hrs_Above_80_lag) 0.0398
###### MIR ~ s(Hrs_Above_75_lag) 0.0213
###### MIR ~ s(Hrs_Below_50_lag) 0.00489
###### MIR ~ s(Avg_HourlyDryBulbTemp_lag) 0.0301
###### MIR ~ s(Avg_HourlyWetBulbTemp_lag) 0.00928
###### MIR ~ s(Hrs_Below_50_current) 0.000593
###### MIR ~ s(Avg_HourlyWetBulbTemperature_current) 0.028
###### MIR ~ s(Avg_HourlyWindSpeed_current) 0.0369
###### MIR ~ s(MEL_Prop) 0.00867


###### I(MIR > 0) ~ s(Avg_HourlyWindSpeed_current) 0.00483
###### I(MIR > 0) ~ s(None.x) 0.000935
###### I(MIR > 0) ~ s(average_daylight_hours) 1.15e-06
######I(MIR > 0) ~ s(Average_MEL) 4.75e-05
######I(MIR > 0) ~ s(MEL_Prop) 5.11e-05
###### I(MIR > 0) ~ s(DSCI_ma) 0.0174

### There is some concern that dividing the data into two parts (observations >0 for part 2) may reduce the power of the analysis.

### Now use a GAM with all records, where MIR >= 0
```{r}
# Response variable
response_var <- "MIR"

# Specified predictors for modeling
specified_predictors <- c("Avg_TMAX", "Total_PRCP", "Hrs_Above_80_lag", "Hrs_Above_75_lag",
                          "Hrs_Below_50_lag", "Hrs_Between_50_86_lag", "Avg_HourlyDryBulbTemp_lag",
                          "Avg_HourlyWetBulbTemp_lag", "Hrs_Below_50_current", "DSCI_fema",
                          "Avg_HourlyRelativeHumidity_current", "Avg_HourlyWetBulbTemperature_current", 
                          "Avg_HourlyWindSpeed_current", "None.x", "None.y",  "DSCI_ma", "average_daylight_hours", 
                          "Average_MEL", "MEL_Prop")

# Initiate lists to store models and summaries
gam_models <- list()
summary_models <- list()

# Loop through each specified predictor
for (predictor in specified_predictors) {
  # Check if predictor exists in the dataset
  if (!predictor %in% names(mir_2part_all)) {
    cat("Skipping", predictor, "because it is not in the dataset.\n")
    next
  }

  # Model with all records
  formula <- as.formula(paste0(response_var, " ~ s(", predictor, ")"))
  model <- gam(formula, data = mir_2part_all, family = gaussian())
  gam_models[[predictor]] <- model
  summary_models[[predictor]] <- summary(model)

  cat("Model for", predictor, "\n")
  
  # Print or plot the summaries here...
  print(summary(model))
  plot(model)
}
```

### The all records model shows the following predictors as significant:
#### MIR ~ s(Hrs_Below_50_lag) 0.0105
#### MIR ~ s(Hrs_Below_50_current) 0.00333
#### MIR ~ s(DSCI_fema) 0.0214
#### MIR ~ s(Avg_HourlyRelativeHumidity_current) 0.00936
#### MIR ~ s(Avg_HourlyWindSpeed_current) 0.00366
#### MIR ~ s(average_daylight_hours) <2e-16
#### MIR ~ s(Average_MEL) 0.0258
#### MIR ~ s(MEL_Prop) <2e-16
## Give the concern with splitting the data further, we will continue with the all records model.

### Now that we have a better idea of the relationships and potential significance.
#### We will have another look at the correlation matrix for the predictors.

```{r}
library(reshape2) # or library(data.table)
library(plotly)

# Assuming continuous_vars is a dataframe with just the continuous variables you want to correlate
# Create a correlation matrix
cor_matrix <- cor(mir_2part %>% 
                    select(Hrs_Above_80_lag, Hrs_Above_75_lag, Hrs_Below_50_lag, 
                           Avg_HourlyDryBulbTemp_lag, Avg_HourlyWetBulbTemp_lag, 
                           Hrs_Below_50_current, Avg_HourlyWetBulbTemperature_current, 
                           Avg_HourlyWindSpeed_current, None.x, average_daylight_hours, 
                           Average_MEL, MEL_Prop, DSCI_ma),
                  use = "complete.obs")  # 'use' parameter handles missing values

# Melt the correlation matrix into a long format
cor_data <- melt(cor_matrix, na.rm = TRUE)

# Ensure cor_data has the expected column names after melting
if (!all(c("Var1", "Var2") %in% names(cor_data))) {
  stop("Var1 and Var2 not found in melted data. Check the melting process.")
}

# Create the interactive heatmap using plotly
heatmap_plot <- plot_ly(
  data = cor_data,
  x = ~Var1,
  y = ~Var2,
  z = ~value,
  type = 'heatmap',
  colorscale = 'Portland'
) %>%
  layout(
    title = 'Correlation Matrix',
    xaxis = list(title = 'Variable 1', tickangle = 45),
    yaxis = list(title = 'Variable 2', autorange = "reversed")
  )

# If you're running this in an interactive R session, this will display the plot
heatmap_plot
```
#### While correlation between predictors is a concern for the GAM model, it is not as much of a concern for the random forest. 
#### I will create a new variable that represents the difference between the average dry bulb temperature and the average wet bulb temperature.
```{r}
# Create a new dataframe by copying mir_2part
mir_2part_updated <- mir_2part

# Add the new variable to the new dataframe
mir_2part_updated$Dry_Wet_Diff <- mir_2part_updated$Avg_HourlyDryBulbTemp_lag - mir_2part_updated$Avg_HourlyWetBulbTemp_lag

# Now, mir_2part remains unchanged, and mir_2part_updated includes the new variable.
```


## 1 part GAM
```{r}
# Load necessary libraries
library(tidymodels)
library(dplyr)
library(mgcv) # For GAM
library(readr) # For read_csv

# Read the dataset
mir_1part_all <- read_csv(here("data", "processed-data", "all_mir_clean_joined.csv"))

# Define predictors for the continuous outcome
predictors_part2 <- c("Hrs_Below_50_lag", 
                      "Hrs_Below_50_current", "Avg_HourlyWetBulbTemperature_current", 
                      "Avg_HourlyWindSpeed_current", "MEL_Prop", "average_daylight_hours", "Average_MEL")

# Prepare the dataset for continuous outcome
mir_1part_all_continuous <- mir_1part_all %>%
  select(MIR, all_of(predictors_part2)) %>%
  na.omit()

# Split the data
set.seed(123)
data_split <- initial_split(mir_1part_all_continuous, prop = 0.75)
train_data_continuous <- training(data_split)
test_data_gam1 <- testing(data_split)

# Calculate the mean of the response variable for the entire dataset
mean_response <- mean(mir_1part_all_continuous$MIR)

# Create predictions using the mean response for the entire dataset
null_predictions <- rep(mean_response, nrow(mir_1part_all_continuous))

# Calculate RMSE for the null model
null_rmse <- sqrt(mean((null_predictions - mir_1part_all_continuous$MIR)^2))
cat("Null Model RMSE (Entire Dataset):", null_rmse, "\n")

# Training for Continuous Outcome
formula_gam1 <- as.formula(paste("MIR ~", paste("s(", predictors_part2, ")", collapse = " + ")))
model_gam1 <- gam(formula_gam1, data = train_data_continuous, family = gaussian())

# Evaluate on Training Set (Continuous)
predictions_train_gam1 <- predict(model_gam1, newdata = train_data_continuous)
rmse_train_gam1 <- sqrt(mean((predictions_train_gam1 - train_data_continuous$MIR)^2))
cat("Training Set RMSE (Continuous Outcome):", rmse_train_gam1, "\n")

# Evaluate on Test Set (Continuous Outcome)
predictions_test_gam1 <- predict(model_gam1, newdata = test_data_gam1)
rmse_test_gam1 <- sqrt(mean((predictions_test_gam1 - test_data_gam1$MIR)^2))
cat("Test Set RMSE (Continuous Outcome):", rmse_test_gam1, "\n")

# Visualize model effects for insights
plot(model_gam1)

### Boot strapping
# Define the bootstrapping statistic function
boot_statistic <- function(data, indices) {
  # Extract the bootstrap sample
  boot_sample <- data[indices, ]
  
  # Fit the GAM model on the bootstrap sample
  gam_boot <- gam(formula_gam1, data = boot_sample, family = gaussian())
  
  # Predict on the out-of-bag (OOB) data
  oob_data <- data[-indices, ]
  predictions_oob <- predict(gam_boot, newdata = oob_data)
  
  # Calculate RMSE on the OOB predictions
  sqrt(mean((predictions_oob - oob_data$MIR)^2))
}

# Perform bootstrapping
set.seed(123)  # Ensure reproducibility
bootstrap_results <- boot(
  data = train_data_continuous,
  statistic = boot_statistic,
  R = 1000  # For example, perform 1000 bootstrap replicates
)

# Extract the bootstrapped RMSE and calculate the mean and standard deviation
bootstrapped_rmse <- bootstrap_results$t
mean_bootstrapped_rmse <- mean(bootstrapped_rmse)
sd_bootstrapped_rmse <- sd(bootstrapped_rmse)

# Display bootstrapped RMSE results
cat("Bootstrapped RMSE: Mean=", mean_bootstrapped_rmse, ", SD=", sd_bootstrapped_rmse, "\n")

# # Calculate confidence intervals for the bootstrapped RMSE
# boot_ci <- boot.ci(bootstrap_results, type = "basic")
# cat("95% CI for RMSE: Lower =", boot_ci$basic[4], ", Upper =", boot_ci$basic[5], "\n")

# Calculate percentile confidence intervals for the bootstrapped RMSE
boot_ci_percentile <- boot.ci(bootstrap_results, type = "perc")

# Display the percentile confidence intervals for the bootstrapped RMSE
cat("95% CI for RMSE (Percentile): Lower =", boot_ci_percentile$percent[4], ", Upper =", boot_ci_percentile$percent[5], "\n")
```


### Residual plots for continuous outcome:
```{r}
# Step 1: Calculate residuals
train_residuals <- residuals(model_gam1, type = "response")

# Step 2: Create residual plot
plot(predictions_train_gam1, train_residuals,
     xlab = "Predicted Values",
     ylab = "Residuals",
     main = "Residual Plot (Training Set)")

# Step 3: Create predicted vs. actual plot for training set
plot(predictions_train_gam1, train_data_continuous$MIR,
     xlab = "Predicted Values",
     ylab = "Actual Values",
     main = "Predicted vs. Actual Plot (Training Set)")

plot(predictions_test_gam1, test_data_gam1$MIR,
     xlab = "Predicted Values",
     ylab = "Actual Values",
     main = "Predicted vs. Actual Plot (Test Set)")

```


## 1 Part Random Forest
```{r}
# Load the dataset
mir_1part_all <- read_csv(here("data", "processed-data", "all_mir_clean_joined.csv"))

# Define predictors for the continuous outcome
predictors <- c("Hrs_Below_50_lag", "Hrs_Below_50_current", 
                "Avg_HourlyWetBulbTemperature_current", "Avg_HourlyWindSpeed_current", 
                "MEL_Prop", "average_daylight_hours", "Average_MEL")

# Prepare the dataset for continuous outcome
train_data_continuous <- mir_1part_all %>%
  select(MIR, all_of(predictors)) %>%
  na.omit()

# Split the data
set.seed(123)
data_split <- initial_split(train_data_continuous, prop = 0.75)
train_data_continuous <- training(data_split)
test_data_continuous <- testing(data_split)

# Define the tuning grid
tuning_grid <- grid_regular(
  mtry(range = c(1, length(predictors))),
  min_n(range = c(5, 25)), # Adjusted based on preliminary insights
  levels = 10 # Increased resolution
)

# Define cross-validation folds
cv_folds_continuous <- vfold_cv(train_data_continuous, v = 5)

# Define the Random Forest model specification
rf_model_spec_continuous <- rand_forest(trees = 500, mtry = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity", seed = 123) %>%
  set_mode("regression")

# Create the recipe
rf_recipe_continuous <- recipe(MIR ~ ., data = train_data_continuous)

# Workflow setup
rf_workflow_continuous <- workflow() %>%
  add_recipe(rf_recipe_continuous) %>%
  add_model(rf_model_spec_continuous)

# Tune the model
tuning_results_continuous <- tune_grid(
  rf_workflow_continuous,
  resamples = cv_folds_continuous,
  grid = tuning_grid,
  metrics = metric_set(yardstick::rmse)
)

# Extract the best hyperparameters
best_hps_continuous <- select_best(tuning_results_continuous, metric = "rmse")

# Fit the final model with the best hyperparameters on the full training dataset
final_fit_continuous <- fit(
  rf_workflow_continuous %>% finalize_workflow(best_hps_continuous),
  data = train_data_continuous
)

# Extract the fit using the appropriate tidymodels function
fitted_model <- extract_fit_engine(final_fit_continuous)

# Check if the extracted model is the correct type
if(inherits(fitted_model, "ranger")) {
    # If it is the correct type, extract feature importance
    importance_matrix <- fitted_model$variable.importance
    feature_importances <- tibble(
        Feature = names(importance_matrix),
        Importance = importance_matrix
    ) %>%
    arrange(desc(Importance))

    # Print the feature importances
    print(feature_importances)

    # Plot the feature importances
    importance_plot <- ggplot(feature_importances, aes(x = reorder(Feature, Importance), y = Importance)) +
        geom_col(fill = "steelblue") +
        coord_flip() +
        labs(title = "Feature Importance from Random Forest", x = "Importance", y = "Features") +
        theme_minimal()

    # Save the plot
    ggsave("feature_importance_rf.pdf", plot = importance_plot, width = 8, height = 6)
} else {
    cat("The extracted object is not a 'ranger' Random Forest model. Please check the model setup and extraction.\n")
}
print(importance_plot)

### Calculate RMSE for the training and testing sets

# Predict on the training and testing set
train_preds <- predict(final_fit_continuous, new_data = train_data_continuous)$.pred
test_preds <- predict(final_fit_continuous, new_data = test_data_continuous)$.pred

# You need to create tibbles that combine the actual values with the predictions
train_results <- tibble(
  truth = train_data_continuous$MIR,
  estimate = train_preds
)

test_results <- tibble(
  truth = test_data_continuous$MIR,
  estimate = test_preds
)

# Now, you can correctly calculate RMSE for both the training and testing sets
train_rmse_result_rf <- rmse(train_results, truth = truth, estimate = estimate)
test_rmse_result_rf <- rmse(test_results, truth = truth, estimate = estimate)

# Display the RMSE results
cat("Training Set RMSE:", train_rmse_result_rf$.estimate, "\n")
cat("Testing Set RMSE:", test_rmse_result_rf$.estimate, "\n")

boot_fit_and_evaluate <- function(data, indices) {
  # Subset the bootstrap sample
  boot_sample <- data[indices, ]
  
  # Predict on the bootstrap sample using the final fitted model
  predictions <- predict(final_fit_continuous, new_data = boot_sample)

  # Ensure predictions are in the correct format, typically need a vector
  # If predictions are not a vector, adjust accordingly
  if (is.data.frame(predictions)) {
    predictions <- predictions$.pred  # Adjust this based on your predict function output structure
  }

  # Calculate the RMSE of predictions
  # Ensure both truth and predictions are either integers or doubles
  results <- tibble(
    truth = as.numeric(boot_sample$MIR),  # Convert to numeric to ensure matching types
    estimate = as.numeric(predictions)  # Convert to numeric to match type
  )
  
  # Compute RMSE and return
  result <- rmse(results, truth = truth, estimate = estimate)
  return(result$.estimate)
}
# Continue with bootstrapping to evaluate model's robustness
set.seed(123) # for reproducibility
bootstrap_results <- boot(
  data = train_data_continuous,
  statistic = boot_fit_and_evaluate,
  R = 100 # number of bootstrap resamples
)
# Calculate the bootstrapped RMSE
mean_rmse_boot_rf <- mean(bootstrap_results$t)
sd_rmse_boot_rf <- sd(bootstrap_results$t)
ci_rmse_boot_rf <- boot.ci(bootstrap_results, type = "perc")  # Percentile interval

# Extracting the correct CI values for percentile CI
lower_bound <- ci_rmse_boot_rf$percent[4]
upper_bound <- ci_rmse_boot_rf$percent[5]


# Output the bootstrapping results
cat("Bootstrapped RMSE: Mean=", mean_rmse_boot_rf, ", SD=", sd_rmse_boot_rf, "\n")
cat("95% CI for RMSE (Percentile): ", lower_bound, "-", upper_bound, "\n")


plot <- autoplot(tuning_results_continuous)

# Save the plot to the specified directory
ggsave(filename = here("results", "figures", "RF_autoplot.png"), plot = plot, width = 10, height = 8)
```
### The random forest model has evidence of overfitting as the RMSE on the test set is higher than the training set. 

## LASSO is not appropriate for non-linear relationships.

### The LASSO model suggests overfitting as well. The RMSE on the test set is higher than the training set.
### We will try modifying the tuning. 
```{r}
library(tidymodels)
library(readr) # For read_csv
library(yardstick) # For metrics

# Load the dataset
mir_1part_all <- read_csv(here("data", "processed-data", "all_mir_clean_joined.csv"))

# Define predictors for the continuous outcome
predictors <- c("Hrs_Below_50_lag", "Hrs_Below_50_current", 
                "Avg_HourlyWetBulbTemperature_current", "Avg_HourlyWindSpeed_current", 
                "MEL_Prop", "average_daylight_hours", "Average_MEL")

# Prepare the dataset for continuous outcome
train_data_continuous <- mir_1part_all %>%
  select(MIR, all_of(predictors)) %>%
  na.omit()

# Split the data
set.seed(123)
data_split <- initial_split(train_data_continuous, prop = 0.75)
train_data_continuous <- training(data_split)
test_data_continuous <- testing(data_split)

# Define a more refined tuning grid for LASSO with a wider range of penalty values
tuning_grid_lasso <- expand.grid(
  penalty = 10^seq(-10, 1, length.out = 50)  # Increase the range and resolution of the penalty parameter
)

# Increase the number of cross-validation folds for a more robust evaluation
cv_folds_continuous <- vfold_cv(train_data_continuous, v = 10)

# Define the LASSO model specification
lasso_model_spec <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

# Create the recipe
#lasso_recipe <- recipe(MIR ~ ., data = train_data_continuous)
lasso_recipe <- recipe(MIR ~ ., data = train_data_continuous) %>%
  update_role(all_of(predictors), new_role = "predictor") %>%
  step_poly(all_of(predictors), degree = 2)

# Workflow setup
lasso_workflow <- workflow() %>%
  add_recipe(lasso_recipe) %>%
  add_model(lasso_model_spec)

# Tune the model using the refined tuning grid and increased folds
tuning_results_lasso <- tune_grid(
  lasso_workflow,
  resamples = cv_folds_continuous,
  grid = tuning_grid_lasso,
  metrics = metric_set(yardstick::rmse)
)

# Extract the best hyperparameters
best_hps_lasso <- select_best(tuning_results_lasso, metric = "rmse")

# Fit the final model with the best hyperparameters on the full training dataset
final_fit_lasso <- fit(
  lasso_workflow %>% finalize_workflow(best_hps_lasso),
  data = train_data_continuous
)

# Predict on the training and testing set using the final LASSO model
train_preds_lasso <- predict(final_fit_lasso, new_data = train_data_continuous)$.pred
test_preds_lasso <- predict(final_fit_lasso, new_data = test_data_continuous)$.pred

# Calculate RMSE for the training and testing sets using the predictions
train_rmse_result_lasso <- yardstick::rmse(data.frame(truth = train_data_continuous$MIR, estimate = train_preds_lasso), truth = truth, estimate = estimate)
test_rmse_result_lasso <- yardstick::rmse(data.frame(truth = test_data_continuous$MIR, estimate = test_preds_lasso), truth = truth, estimate = estimate)

# Display the RMSE results
cat("Training Set RMSE (LASSO):", train_rmse_result_lasso$.estimate, "\n")
cat("Testing Set RMSE (LASSO):", test_rmse_result_lasso$.estimate, "\n")

# Visualize the tuning results
autoplot(tuning_results_lasso)


```
### There still seems to be overfitting. The RMSE on the test set is higher than the training set.
### We will try a different model

# Generalized Additive Models (GAMs) with sparsity
### GAMs are flexible models that can capture non-linear relationships between predictors and the outcome.
### Acts similar to LASSO adding a penalty similar to LASSO to enforce sparsity and variable selection
```{r}
library(tidymodels)
library(mgcv)
library(readr) # For read_csv

# Load the dataset
mir_1part_all <- read_csv(here("data", "processed-data", "all_mir_clean_joined.csv"))

# Prepare the dataset for continuous outcome
train_data_continuous <- mir_1part_all %>%
  select(MIR, Hrs_Below_50_lag, Hrs_Below_50_current, Avg_HourlyWetBulbTemperature_current, 
         Avg_HourlyWindSpeed_current, MEL_Prop, average_daylight_hours, Average_MEL) %>%
  na.omit()

# Split the data
set.seed(123)
data_split <- initial_split(train_data_continuous, prop = 0.75)
train_data_continuous <- training(data_split)
test_data_continuous <- testing(data_split)

# Define cross-validation folds
cv_folds_continuous <- vfold_cv(train_data_continuous, v = 5)

# Placeholder for storing CV results
cv_results <- vector("list", length(cv_folds_continuous$splits))

# Loop through CV folds
for (i in seq_along(cv_folds_continuous$splits)) {
  split <- cv_folds_continuous$splits[[i]]
  
  # Extract training and validation data for this fold
  train_split <- training(split)
  valid_split <- testing(split)
  
  # Fit a GAM with sparsity
  gam_model <- gam(MIR ~ s(Hrs_Below_50_lag) + s(Hrs_Below_50_current) + 
                     s(Avg_HourlyWetBulbTemperature_current) + s(Avg_HourlyWindSpeed_current) + 
                     s(MEL_Prop) + s(average_daylight_hours) + s(Average_MEL),
                   data = train_split, method = "REML", select = TRUE)
  
  # Make predictions on the validation set
  preds <- predict(gam_model, newdata = valid_split, type = "response")
  
  # Create a tibble that includes both the actual values and predictions
  results_tibble <- tibble(
    truth = valid_split$MIR,
    estimate = preds
  )
  
  # Calculate RMSE for this fold using the results tibble
  cv_results[[i]] <- rmse(results_tibble, truth = truth, estimate = estimate)
}

# First, verify the structure of the first element to ensure we understand it correctly
print(cv_results[[1]])

# If cv_results[[1]] is indeed a tibble with an .estimate column, proceed with the following:
avg_rmse_GAMS <- mean(sapply(cv_results, function(x) x$.estimate))

# Display the average RMSE
cat("Average CV RMSE:", avg_rmse_GAMS, "\n")

# Fit the final GAM model on the full training data
final_gam_model <- gam(MIR ~ s(Hrs_Below_50_lag) + s(Hrs_Below_50_current) + 
                         s(Avg_HourlyWetBulbTemperature_current) + s(Avg_HourlyWindSpeed_current) + 
                         s(MEL_Prop) + s(average_daylight_hours) + s(Average_MEL),
                       data = train_data_continuous, method = "REML", select = TRUE)

# Create predictions for the training set using the final GAM model
train_preds <- predict(final_gam_model, newdata = train_data_continuous, type = "response")

# Create a tibble that includes both the actual values and predictions for the training set
train_results <- tibble(
  truth = train_data_continuous$MIR,  # Actual values
  estimate = train_preds  # Predicted values
)

# Calculate RMSE for the training set
train_rmse_result_GAMS <- rmse(train_results, truth = truth, estimate = estimate)
  
# Make predictions for the test set using the final GAM model
test_preds <- predict(final_gam_model, newdata = test_data_continuous, type = "response")

# Create a tibble that includes both the actual values and predictions for the test set
test_results <- tibble(
  truth = test_data_continuous$MIR,  # Actual values
  estimate = test_preds  # Predicted values
)

# Calculate RMSE for the test set
test_rmse_result_GAMS <- rmse(test_results, truth = truth, estimate = estimate)

# Display the RMSE for the training set
cat("Training Set RMSE:", train_rmse_result_GAMS$.estimate, "\n")
# Display the RMSE for the test set
cat("Test Set RMSE:", test_rmse_result_GAMS$.estimate, "\n")

# Calculate AIC for the final GAM model
aic_value <- AIC(final_gam_model)

# Display the AIC value
cat("AIC for the final GAM model:", aic_value, "\n")

```
# Gradient Boosting Machine (GBM)
```{r}

library(tidymodels)
library(dplyr)
library(yardstick)
library(ranger) # For Random Forest
library(boot) # For bootstrapping

# Load the dataset
mir_1part_all <- read_csv(here("data", "processed-data", "all_mir_clean_joined.csv"))

# Define predictors for the continuous outcome
predictors <- c("Hrs_Below_50_lag", "Hrs_Below_50_current", 
                "Avg_HourlyWetBulbTemperature_current", "Avg_HourlyWindSpeed_current", 
                "MEL_Prop", "average_daylight_hours", "Average_MEL")

# Prepare the dataset for continuous outcome
train_data_continuous <- mir_1part_all %>%
  select(MIR, all_of(predictors)) %>%
  na.omit()

# Split the data
set.seed(123)
data_split <- initial_split(train_data_continuous, prop = 0.75)
train_data_continuous <- training(data_split)
test_data_continuous <- testing(data_split)

# Define the tuning grid
tuning_grid <- grid_regular(
  trees(range = c(50, 500)), # Adjusted for GBM
  tree_depth(range = c(1, 5)), # Shallow trees
  learn_rate(range = c(0.01, 0.1)), # Learning rate for gradient descent
  min_n(range = c(5, 25)), # Adjusted based on preliminary insights
  levels = 10 # Increased resolution
)

# Define cross-validation folds
cv_folds_continuous <- vfold_cv(train_data_continuous, v = 5)

# Define the GBM model specification
gbm_model_spec_continuous <- boost_tree(trees = tune(), tree_depth = tune(), learn_rate = tune(), min_n = tune()) %>%
  set_engine("xgboost", objective = "reg:squarederror", nthread = parallel::detectCores()) %>%
  set_mode("regression")

# Create the recipe
gbm_recipe_continuous <- recipe(MIR ~ ., data = train_data_continuous)

# Workflow setup
gbm_workflow_continuous <- workflow() %>%
  add_recipe(gbm_recipe_continuous) %>%
  add_model(gbm_model_spec_continuous)

# Tune the model
tuning_results_continuous <- tune_grid(
  gbm_workflow_continuous,
  resamples = cv_folds_continuous,
  grid = tuning_grid,
  metrics = metric_set(yardstick::rmse)
)

# Extract the best hyperparameters
best_hps_continuous <- select_best(tuning_results_continuous, metric = "rmse")

# Fit the final model with the best hyperparameters on the full training dataset
final_fit_continuous <- fit(
  gbm_workflow_continuous %>% finalize_workflow(best_hps_continuous),
  data = train_data_continuous
)


### Calculate RMSE for the training and testing sets

# Predict on the training and testing set
train_preds <- predict(final_fit_continuous, new_data = train_data_continuous)$.pred
test_preds <- predict(final_fit_continuous, new_data = test_data_continuous)$.pred

# You need to create tibbles that combine the actual values with the predictions
train_results <- tibble(
  truth = train_data_continuous$MIR,
  estimate = train_preds
)

test_results <- tibble(
  truth = test_data_continuous$MIR,
  estimate = test_preds
)

# Now, you can correctly calculate RMSE for both the training and testing sets
train_rmse_result_gbm <- rmse(train_results, truth = truth, estimate = estimate)
test_rmse_result_gbm <- rmse(test_results, truth = truth, estimate = estimate)

# Display the RMSE results
cat("Training Set RMSE:", train_rmse_result_gbm$.estimate, "\n")
cat("Testing Set RMSE:", test_rmse_result_gbm$.estimate, "\n")

# Define a function to fit the model and calculate RMSE
boot_fit_and_evaluate <- function(data, indices) {
  # Extract the bootstrap sample
  bootstrap_data <- data[indices, ]
  
  # Fit the model on the bootstrap sample
  model <- gbm(MIR ~ ., data = bootstrap_data, distribution = "gaussian",
               n.trees = 1000, interaction.depth = 4, shrinkage = 0.01)
  
  # Predict on the original dataset
  preds <- predict(model, newdata = data, n.trees = 1000)
  
  # Compute RMSE
  rmse <- sqrt(mean((preds - data$MIR)^2))
  
  # Return RMSE
  return(rmse)
}


# Continue with bootstrapping to evaluate model's robustness
set.seed(123) # for reproducibility
bootstrap_results <- boot(
  data = train_data_continuous,
  statistic = boot_fit_and_evaluate,
  R = 100 # number of bootstrap resamples
)

# Calculate the bootstrapped RMSE
mean_rmse <- mean(bootstrap_results$t)
sd_rmse <- sd(bootstrap_results$t)
ci_rmse <- boot.ci(bootstrap_results, type = "perc")  # Percentile interval

# Extracting the correct CI values for percentile CI
lower_bound <- ci_rmse$percent[4]
upper_bound <- ci_rmse$percent[5]

# Output the bootstrapping results
cat("Bootstrapped RMSE: Mean=", mean_rmse, ", SD=", sd_rmse, "\n")
cat("95% CI for RMSE (Percentile): ", lower_bound, "-", upper_bound, "\n")

plotGBM <- autoplot(tuning_results_continuous)

#save plot as a png
ggsave(filename = here("results", "figures", "GBM_autoplot.png"), plot = plotGBM, width = 10, height = 8)

```

# SUmmarize the RSME values
```{r}
rmse_table <- data.frame(
  Model = c("Null Model", "Generalized Additive Model", "Random Forest", "LASSO",
            "Generalized Additive Model with Sparsity", "Gradient Boosting Machine"),
  Train_RMSE = c(null_rmse, rmse_train_gam1, train_rmse_result_rf$.estimate, 
                 train_rmse_result_lasso$.estimate, train_rmse_result_GAMS$.estimate, 
                 train_rmse_result_gbm$.estimate),
  Test_RMSE = c(null_rmse, rmse_test_gam1, test_rmse_result_rf$.estimate, 
                test_rmse_result_lasso$.estimate, test_rmse_result_GAMS$.estimate, 
                test_rmse_result_gbm$.estimate)
)
# Sort the table by Test_RMSE column in descending order
rmse_table <- rmse_table[order(-rmse_table$Test_RMSE), ]

# Print the sorted table
print(rmse_table)

#save as an RDS file
saveRDS(rmse_table, here("results", "tables", "rmse_table.rds"))

# Use gt to create a styled table
gt_table <- gt(rmse_table) %>%
  tab_header(
    title = "RMSE Comparison of Models",
    subtitle = "Sorted by Test RMSE in descending order"
  ) %>%
  cols_label(
    Model = "Model",
    Train_RMSE = "Training RMSE",
    Test_RMSE = "Testing RMSE"
  )

# Define the file path using 'here'
png_path <- here("results", "tables", "rmse_table.png")

# Save the gt table as a PNG
gtsave(gt_table, filename = png_path)
```














