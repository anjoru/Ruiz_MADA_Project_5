---
title: "MADA Project Part 2"
subtitle: "loading, cleaning, exploring the data"
author: "Andrew Ruiz"
date: 2024-02-23
format: html
editor: source
---

# Introduction

## This file contains the process for loading, cleaning and exploring the datasets.
### There is also a little visualizing data


### Use this code to ensure you have all the packages installed before proceeding.
#### This code will check if the packages are installed and install them if they are not.
```{r}
# List of packages required for the project
required_packages <- c(
  "readxl", "httr", "dplyr", "tidyr", "skimr", "here", "readr",
  "naniar", "ggplot2", "sf", "topicmodels", "openxlsx", "lubridate",
  "knitr", "kableExtra", "webshot2", "magick", "leaflet", "htmlwidgets",
  "suncalc", "stringr", "ISOweek"
)

# Function to check and install missing packages
install_missing_packages <- function(packages) {
  for (pkg in packages) {
    if (!require(pkg, character.only = TRUE, quietly = TRUE)) {
      install.packages(pkg)
      library(pkg, character.only = TRUE)
    }
  }
}

# Install missing packages
install_missing_packages(required_packages)
```

# Loading libraries
## This list contains the specific versions used in the script.
```{r, message=FALSE, warning=FALSE}
library(readxl) # for loading Excel files (version 1.4.0)
library(httr) # for getting data from the web (version 1.4.2)
library(dplyr) # for data processing/cleaning (version 1.0.7)
library(tidyr) # for data processing/cleaning (version 1.1.3)
library(skimr) # for visualizing data (version 2.1.3)
library(here) # allows relative pathways for data (version 1.0.1)
library(readr) # reading and writing data to and from CSV, TSV, and fixed-width files (version 2.0.1)
library(naniar) # tools to work with missing data (version 0.6.1)
library(ggplot2) # plot creation (version 3.3.5)
library(sf) # standardized way to encode spatial data (version 1.0-2)
library(topicmodels) # tools for topic modeling on text data (version 0.2-12)
library(openxlsx) # Facilitates reading from and writing to .xlsx files (version 4.2.4)
library(lubridate) # Simplifies the work with dates and times (version 1.7.10)
library(knitr) # Allows for dynamic report generation in R (version 1.33)
library(kableExtra) # generates simple HTML or LaTeX tables (version 1.3.4)
library(webshot2) # tools to take screenshots of web pages (version 0.0.2)
library(magick) # advanced image processing in R (version 2.7.3)
library(leaflet) # Enables the creation of interactive maps (version 2.0.4.1)
library(htmlwidgets) # Facilitates the creation of R bindings to JavaScript libraries (version 1.5.4)
library(suncalc) # Calculates sunset and sunrise times based on location (version 0.5.1)
library(stringr) # Makes string manipulation easier and more consistent (version 1.5.1)
library(ISOweek) # Calculates week number according to ISO 8601 standards (version 0.6-2)
library(purrr)
```

# Loading data
## Mosquito trap data
### This dataset contains trap counts by species for all mosquito traps set in the Bristol County from 2007-2023 this dataset will undergo extensive processing below.
#### The dataset was obtained though and open records request with the Commonwealth of MA.
##### we will read the data now and process it later in this script
```{r trap-data-import}
# Construct the path to the Excel file
file_path_mos <- here("data", "raw-data", "MOSQ_MADA.xlsx")
file_exists_check <- file.exists(file_path_mos)
file_path_mos <- here("data", "raw-data", "MOSQ_MADA.xlsx")

#file_exists_check <- file.exists("/Users/andrewruiz/MADA_course/RUIZ-MADA-project/data/raw-data/MOSQ_MADA.xlsx")
print(file_exists_check)  # Should return TRUE if the path is correct
print(file_path_mos)

print(file_exists_check)  # This should return TRUE if the file path is correct
# import the data to R and store it in a dataframe named "mosquito"
mosquito_raw <- read_excel(file_path_mos)

#load the first few rows of data to verify the data loaded as expected
head(mosquito_raw)
```

## Virus testing of the trap count data
### This dataset is identical to MOSQ_MADA except that it contains virus test results.
```{r}
# Read the dataset
pcr_data <- read_excel(here("data", "raw-data", "virus_iso.xlsx"))

# Add week number based on the collection data
# This code ensures that the ISO 8601 standard is applied to the week number calculation
# Week 1 of any year is the week that contains January 4th, or equivalently, it's the week that contains the first Thursday of January.
# Weeks start on Monday and end on Sunday.
# The last week of the year, week 52 or 53, is the one that contains December 28th.
# we will use week_num to join tables
# the format will be YYYY-WW. Week 8 of 2019 will look like: 2019-08
pcr_data <- pcr_data %>%
  mutate(
    # Directly format the collection date into "YYYY-WW"
    week_num = paste0(year(`Collection Date`), "-", 
                      sprintf("%02d", isoweek(`Collection Date`)))
  )

# Save as RDS
saveRDS(pcr_data, here("data", "processed-data", "rds", "pcr_data.rds"))

# Save as CSV
write_csv(pcr_data, here("data", "processed-data", "pcr_data.csv"))

```

## Mosquito trap sites
### Location for the mosquito trap sites
```{r}
# Construct the path to the file
file_path_trap = here("data", "raw-data", "BCMCP_trap_sites_2021.xlsx")

# import the data to R and store it in a dataframe named "trap_location"
trap_location = read_excel(file_path_trap)

#load the first few rows of data to verify the data loaded as expected
head(trap_location)

#The last 3 columns have missing data. However, they are not important for the purposes of this project
# there is not a need to delete the records
# this is a free text field that provides notes about the location.
# they are not needed so we will delete them
trap_location_selected <- trap_location %>%
  select(`Trap Site Address`, MCD, Location, DecLat, DecLong)
trap_location_selected

# Define the path for the trap locations selected dataset RDS
trap_location_selected_path_rds <- here("data", "processed-data", "rds", "trap_sites_selected.rds")

# Save the trap_location_selected dataframe as an RDS file
saveRDS(trap_location_selected, trap_location_selected_path_rds)

#save as csv
trap_location_selected_path_csv <- here("data", "processed-data", "trap_sites_selected.csv")
write_csv(trap_location_selected, trap_location_selected_path_csv)

```

## Mosquito virus data

### The data includes all the positive PCR results for EEE and WNV for all mosquitoes submitted for testing
### These files were obtained from: https://www.mass.gov/lists/arbovirus-surveillance-plan-and-historical-data
### The virus isolation data is stored in separate spreadsheets by year from 2014 to 2020
### This code will load all the files, combine them, and then save the combined file in the "raw data" folder
```{r}
#Define the path to the folder with Excel files
folder_path <- here("data", "raw-data", "virus_data")

#List all Excel files (.xls and .xlsx)
file_paths <- list.files(path = folder_path, pattern = "\\.xls[x]*$", full.names = TRUE)

#Read each file into a list of data frames
data_list <- lapply(file_paths, read_excel)

#Check the names of the files read
print(basename(file_paths))

#Combine the datasets
combined_virus_data <- bind_rows(data_list)

# Define the path for the combined dataset
combined_path <- here("data", "processed-data", "combined_virus_data.csv")

# Save the combined data frame as a CSV file
write.csv(combined_virus_data, combined_path, row.names = FALSE)

# Define the path for the combined dataset RDS
combined_path_rds <- here("data", "processed-data", "rds", "combined_virus_data.rds")

# Save the combined data frame as an RDS file
saveRDS(combined_virus_data, combined_path_rds)

# View the first few rows to confirm it's loaded correctly
head(combined_virus_data)
```


## Weather data
### temperature and precipitation are important factors in mosquito population dynamics
### This dataset contains daily weather summaries from the Taunton, MA Airport from 2007-2023
#### The data was obtained from the NOAA National Centers for Environmental Information
```{r}

# Construct the path to the file
file_path_wx <- here("data", "raw-data", "weather_airport.csv")

# Import the data to R and store it in a dataframe named "wx"
wx <- read.csv(file_path_wx)

# Check the structure of the DATE column to determine if there are any unexpected values
str(wx$DATE)

# Check for any missing values in the DATE column
sum(is.na(wx$DATE))

# If there are no missing values, proceed with the mutation
wx <- wx %>%
  mutate(
    DATE = ymd(DATE),  # Parse the DATE column from "year-month-day"
    week_num = paste0(year(DATE), "-", sprintf("%02d", isoweek(DATE)))  # Create week_num
  )

# Load the first few rows of data to verify the data loaded as expected
head(wx)

#the columns below are the only ones needed for this project. 
wx_selected <- wx %>%
  dplyr::select(STATION, NAME, DATE, PRCP, TMAX, week_num)

# Define the path for the RDS file
output_path_wx <- here("data", "processed-data", "rds", "wx_selected.rds")
output_csv_wx <- here("data", "processed-data", "wx_selected.csv")
# Save the wx_selected dataframe as an RDS file
saveRDS(wx_selected, output_path_wx)
write.csv(wx_selected, output_csv_wx, row.names = FALSE)
head(wx_selected)
```

# hourly weather
## This dataset contains hourly weather data from the Taunton, MA Airport from 2007-2023
### The data was obtained from the NOAA National Centers for Environmental Information
```{r}
hourly_wx_path <- here("data", "raw-data", "hourly_wx.csv")

# Ensure hourly_wx is read if not already done
hourly_wx <- read_csv(hourly_wx_path, show_col_types = FALSE)
str(hourly_wx)
hourly_wx_clean <- hourly_wx %>%
  mutate(Year = year(DATE),
         ISO_Week = isoweek(DATE),
         week_num = sprintf("%d-%02d", Year, ISO_Week)) %>%
  filter(REPORT_TYPE...3 != "SOD", REPORT_TYPE...3 != "SOM", REPORT_TYPE...3 != "AUTO") %>%
  select(STATION, DATE, HourlyDryBulbTemperature, HourlyRelativeHumidity, HourlyWetBulbTemperature, HourlyWindSpeed, week_num)

# Define the path for the RDS and CSV files
rds_path_hourly_wx <- here("data", "processed-data", "rds", "hourly_wx_clean.rds")
csv_path_hourly_wx <- here("data", "processed-data", "hourly_wx_clean.csv")

# Save the cleaned dataframe as RDS and CSV files
saveRDS(hourly_wx_clean, rds_path_hourly_wx)  # Save as RDS
write.csv(hourly_wx_clean, csv_path_hourly_wx, row.names = FALSE) # Save as CSV

```
## Drought data
### These datasets were obtained from the US Drought Monitor
#### The U.S. Drought Monitor is produced through a partnership between the National Drought Mitigation Center at the University of Nebraska-Lincoln, the United States Department of Agriculture and the National Oceanic and Atmospheric Administration
##### This data is organized by geographic area and week
##### It includes 4 categories. D4 - Exceptional Drought (0-2 percentile), D3 - Extreme Drought (2-5 percentile), D2 - Severe Drought (5-10 percentile), D1 - Moderate Drought (10-20 percentile), D0 - Abnormally Dry (20-30 percentile), None or 0 - Neither Wet or Dry
##### It provides the percent area covered by each category. 
```{r}
# read file with cumulative drought data by county for all MA counties for the years 2006-2023
drought_ma_counties = read_csv(here("data", "raw-data", "comp_stat_ma_counties.csv"), show_col_types = FALSE)

# read file with drought data by county for FEMA Region 1-4 for the years 2007-2023
drought_fema1_4 = read_csv(here("data", "raw-data", "comp_stat_fema1_4.csv"), show_col_types = FALSE)

# Add week number based on MapDate for both files
# This code ensures that the ISO 8601 standard is applied to the week number calculation
# Week 1 of any year is the week that contains January 4th, or equivalently, it's the week that contains the first Thursday of January.
# Weeks start on Monday and end on Sunday.
# The last week of the year, week 52 or 53, is the one that contains December 28th.
# we will use week_num to join tables
# the format will be YYYY-WW. Week 8 of 2019 will look like: 2019-08

# Additionally, this code calculates the Drought Severity and Coverage Index (DSCI) for each dataset
# The DSCI is calculated as the sum of D0, D1, D2, D3, and D4 categories
# This index provides a measure of the overall drought severity and can range from 0-500,
# where 0 is no drought and 500 is the most severe drought over the entire area

drought_ma_counties <- drought_ma_counties %>%
  mutate(
    # Convert MapDate from a numeric format to a Date object
    MapDate = as.Date(as.character(MapDate), format = "%Y%m%d"),
    # Create a week_num column formatted as "YYYY-WW"
    week_num = paste0(year(MapDate), "-", sprintf("%02d", isoweek(MapDate))),
    # Calculate the DSCI, assuming D0 to D4 are numeric
    DSCI_ma = rowSums(select(., D0:D4), na.rm = TRUE)
  )


drought_fema1_4 <- drought_fema1_4 %>%
 mutate(
    # Convert MapDate from a numeric format to a Date object
    MapDate = as.Date(as.character(MapDate), format = "%Y%m%d"),
    # Create a week_num column formatted as "YYYY-WW"
    week_num = paste0(year(MapDate), "-", sprintf("%02d", isoweek(MapDate))),
    # Calculate the DSCI, assuming D0 to D4 are numeric
        DSCI_fema = rowSums(select(., c("D0", "D1", "D2", "D3", "D4")))
  )

# Define the path for the RDS and CSV files
rds_path_drought_ma_counties<- here("data", "processed-data", "rds", "drought_ma_counties.rds")
csv_path_drought_ma_counties <- here("data", "processed-data", "drought_ma_counties.csv")
# Save the dataframe as an RDS and CSV files
saveRDS(drought_ma_counties, rds_path_drought_ma_counties)
write.csv(drought_ma_counties, csv_path_drought_ma_counties, row.names = FALSE)

# Define the path for the RDS and CSV files
rds_path_drought_fema1_4<- here("data", "processed-data", "rds", "drought_fema1_4.rds")
csv_path_drought_fema1_4 <- here("data", "processed-data", "drought_fema1_4.csv")
# Save the wx_selected dataframe as an RDS and CSV files
saveRDS(drought_fema1_4, rds_path_drought_fema1_4)
write.csv(drought_fema1_4, csv_path_drought_fema1_4, row.names = FALSE)
```

## Human and veterinary cases of eastern equine encephalitis and West Nile virus

### these were extracted from annual mosquito reports publised here: https://www.mass.gov/lists/arbovirus-surveillance-plan-and-historical-data

### human cases are listed by county only. animal cases are listed by town/city (with the exception of 1 year)

```{r}
# Construct the path to the human case data file
file_path_human <- here("data", "raw-data", "human_cases.xlsx")

# import the data to R and store it in a dataframe named "cases_human"
cases_human <- read_excel(file_path_human)

# Save 'cases_human' as an RDS file
output_path_rds_human <- here("data", "processed-data", "rds", "human_cases_processed.rds")
saveRDS(cases_human, output_path_rds_human)

#load the first few rows of data to verify the data loaded as expected
head(cases_human)

# Construct the path to the veterinary case data file
file_path_vet <- here("data", "raw-data", "animal_cases.xlsx")

# import the data to R and store it in a dataframe named "cases_human"
cases_vet <- read_excel(file_path_vet)

# Save 'cases_vet' as an RDS file
output_path_rds_vet <- here("data", "processed-data", "rds", "vet_cases_processed.rds")
saveRDS(cases_vet, output_path_rds_vet)

#load the first few rows of data to verify the data loaded as expected
head(cases_vet)
#date of onset is missing for some records, however, they are not needed for this project. 
```


# Processing and cleaning the Mosquito trap data
### Examine the variables and change all column names to lower case
```{r}
str(mosquito_raw)

# Set column names to lowercase
names(mosquito_raw) <- tolower(names(mosquito_raw))

# Check the first few rows to confirm changes
head(mosquito_raw)
```

## we will examine the dataset before making changes.
### first we will Identify missing data
```{r}
sapply(mosquito_raw, function(x) sum(is.na(x)))
```

#### The majority of the missing data is in the "Submitted for Testing" variable. This is not a crucial variable, so we can keep those records. Missing data in that variable generally means 0 mosquitoes were captured in a trap. We will remove the rows with missing data in the other columns in th code chunks below.

### The Massachussetts Department of Public Health adpoted a taxonomy change before it was fully vetted. The change divided the genus *Aedes* into a new genus: *Ochlerotatus*. To align with current accepted standards, I will change them back to *Aedes*.

```{r}
# Get unique values using dplyr
unique_genus_values <- mosquito_raw %>%
  distinct(genus) %>%
  pull(genus)

# Print the unique genus values
print(unique_genus_values)

# Change records where genus is "Ochlerotatus" to "Aedes"
mosquito_raw$genus[mosquito_raw$genus == "Ochlerotatus"] <- "Aedes"

# Check the first few rows to confirm changes, or use unique() to see the change in genus values
unique(mosquito_raw$genus)
```

### Execute proposed from above

```{r}
# Create a copy of the dataframe to avoid modifying the original
mosquito_modified <- mosquito_raw

# Change records where genus is "Ochlerotatus" to "Aedes" to adhere to current taxonomy standards
mosquito_modified$genus[mosquito_modified$genus == "Ochlerotatus"] <- "Aedes"

# Ensure Collection Date is in Date format 
mosquito_modified$`collection date` <- as.Date(mosquito_modified$`collection date`)

# Convert all species code values to uppercase
# This will minimize issues were data as entered incorrectly
# for example Mel, mel, and MEL
mosquito_modified <- mosquito_modified %>%
  mutate(`species code` = toupper(`species code`))

# Remove rows with NA values in the remaining columns of the modified dataframe, 
# except for "Submitted for Testing" which we'll handle separately
# when Submitted for testing is NA or missing, it indicates that there were no mosquitoes captured during a trap event.
# This is common at the beginning and end of season and also when a trap malfunctions
temp_columns <- names(mosquito_modified)
temp_columns <- temp_columns[temp_columns != "submitted for testing"]
mosquito_modified <- na.omit(mosquito_modified, cols = temp_columns)

# Add "week_num" column
# This code ensures that the ISO 8601 standard is applied to the week number calculation
# Week 1 of any year is the week that contains January 4th, or equivalently, it's the week that contains the first Thursday of January.
# Weeks start on Monday and end on Sunday.
# The last week of the year, week 52 or 53, is the one that contains December 28th.
# we will use week_num to join tables
# the format will be YYYY-WW. Week 8 of 2019 will look like: 2019-08
mosquito_modified <- mosquito_modified %>%
  mutate(
    week_num = paste0(year(`collection date`), "-", sprintf("%02d", isoweek(`collection date`)))
  )
# Check the structure of the final cleaned dataframe
str(mosquito_modified)

# before filtering the dates to have a cleaned dataset with all the records.
write_csv(mosquito_modified, here("data", "processed-data", "mosquito_clean_all.csv"))
saveRDS(mosquito_modified, here("data", "processed-data", "rds", "mosquito_clean_all.rds"))
```

## Reformat the collection data

### The Massachusetts Department of Public Health and US CDC require that all mosquito control districts submit their colelction data in a specific format. This format divides each trap event into separate rows based on mosquito species. However, this format is not useful for certain calculations where a zero count for a species in not explicitly recorded. Even if you sort by species and calculate average count per trap, the average could be inflated since the records will not include trap events with a zero count of that species.

### In order to correct this, I will pivot the data and create a column for every unique "species code" and sum the "pool size" for each species code. The resulting table will have one row for every trap event -a trap event is when "town", "date of collection", "trap type" are all the same.

### The resulting data frame will rectangle in shape and contain 1 row per trap event with all possible species as a separate column.


```{r}

# Step 1: Summarize the data
mosquito_summarized <- mosquito_modified %>%
  group_by(`collection date`, town, `trap type`, `species code`, mcd, `submitted for testing`) %>%
  summarise(pool_size_sum = sum(`pool size`, na.rm = TRUE), .groups = 'drop')

# Step 2: Pivot to wide format
wide_data <- mosquito_summarized %>%
  pivot_wider(
    names_from = `species code`,
    values_from = pool_size_sum,
    id_cols = c(`collection date`, town, `trap type`, mcd)
  )

# Define species_columns after pivoting
species_columns <- setdiff(names(wide_data), c("collection date", "town", "trap type", "mcd"))

# Ensure there's no duplication in the conversion attempt
# Step 3: Convert list columns to numeric, replacing NULL or list elements without values with 0
# Utilizing sapply for a direct approach to handle potential list-columns
wide_data <- wide_data %>%
  mutate(across(.cols = all_of(species_columns), 
                .fns = ~ map_dbl(.x, function(item) {
                  # Handle NULL items directly
                  if (is.null(item)) {
                    return(0)
                  # Handle vectors: convert NA to 0, and take the first item if it's a vector
                  } else if (length(item) > 1 || is.na(item)) {
                    item[is.na(item)] <- 0
                    return(as.numeric(item[1]))
                  } else {
                    return(as.numeric(item))
                  }
                })))
# Step 4: Add week_num column
mosquito_wide <- wide_data %>%
 mutate(
    week_num = paste0(year(`collection date`), "-", sprintf("%02d", isoweek(`collection date`)))
  )

# Output the transformed data to inspect its structure
str(mosquito_wide)

# Save the new file
# Specify the file path using here()
file_path_wide <- here("data", "processed-data", "mosquito_wide_all.csv")

# Save the wide-format data to the specified path
write_csv(mosquito_wide, file_path_wide)

# Define the path for the wide-format dataset RDS
file_path_wide_rds <- here("data", "processed-data", "rds", "mosquito_wide_all.rds")

# Save the wide-format data as an RDS file
saveRDS(mosquito_wide, file_path_wide_rds)

```


# Exploring the Data

##Study location Below is a map of the mosquito trap sites. This is an interactive map and will only work on web or other html-friendly platforms.

```{r}
# Create the map
map_study <- leaflet(trap_location) %>%
  addTiles() %>%  # This adds the default OpenStreetMap tiles
  addMarkers(lng = ~DecLong, lat = ~DecLat, popup = ~Location)  # Customize the popup content as needed

# Display the map
map_study

# Specify the path and name of your HTML file
file_path_map <- here("results", "figures", "map_study.html")

# Save the map
saveWidget(map_study, file_path_map, selfcontained = TRUE)
```

## EEE in mosquitoes and mammals

### To give an idea of when EEE virus spilled over from mosquitoes and birds and into mammal populations, lets take a look at the data. Let's see when the first EEE isolations were found in mosquitoes each year. Let's also look at the years that human and veterinary cases occured.

### From the table, we see that human or othe mammal cases were recorded in 2014, 2018, 2019, and 2020. 2019 had the highest case counts for both humans and other mammals. **Because of this, we will pay close attention to 2019 through out the exploration.**

```{r}

# virus_data, cases_human, and cases_vet data frames 
# Prepare virus_data: Find the first positive EEE test date by year
first_positive_eee_test_by_year <- combined_virus_data %>%
  filter(Virus == "EEE") %>%
  mutate(Year = year(as.Date(`Collection Date`))) %>%
  group_by(Year) %>%
  summarise(First_Positive_Test_Date = min(`Collection Date`)) %>%
  ungroup() %>%
  arrange(Year)

# Prepare cases_human: Summarize human EEE cases by year
human_cases_by_year <- cases_human %>%
  filter(`Virus Result` == "EEE") %>%
  mutate(Year = year(`Onset Date`)) %>%
  group_by(Year) %>%
  summarise(Human_Cases = n()) %>%
  ungroup() %>%
  arrange(Year)

# Prepare cases_vet: Summarize animal EEE cases by year
animal_cases_by_year <- cases_vet %>%
  filter(virus == "eee") %>%
  group_by(year) %>%
  summarise(Animal_Cases = n()) %>%
  ungroup() %>%
  arrange(year)

# Ensure 'Year' column is numeric across all data frames for compatibility
# Note: This step might already be covered by the mutations above but included here for clarity
first_positive_eee_test_by_year$Year <- as.numeric(first_positive_eee_test_by_year$Year)
animal_cases_by_year$year <- as.numeric(animal_cases_by_year$year) # Make sure year is numeric and named consistently

# Merge the summaries into one combined data frame
combined_eee_data <- first_positive_eee_test_by_year %>%
  left_join(human_cases_by_year, by = "Year") %>%
  left_join(animal_cases_by_year, by = c("Year" = "year")) # Ensure correct column names are used for joining

# Replace NA values with 0 for Human_Cases and Animal_Cases
combined_eee_data$Human_Cases[is.na(combined_eee_data$Human_Cases)] <- 0
combined_eee_data$Animal_Cases[is.na(combined_eee_data$Animal_Cases)] <- 0

# Creating the table with centered headings and data
kable_table_isolation_case <- kable(combined_eee_data, "html", 
                     col.names = c("Year", "First Positive EEE Test Date", "Human EEE Cases", "Animal EEE Cases"),
                     caption = "Annual Summary of EEE Virus Activity",
                     align = c('c','c','c','c')) %>% # This aligns all columns' data to center
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                full_width = F, position = "center") %>%
  add_header_above(c(" " = 1, "EEE Virus Detection and Cases" = 3)) %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(2, width = "8em") %>% # Adjusted width without altering alignment
  column_spec(3, width = "5em", bold = TRUE, color = "red") %>%
  column_spec(4, width = "5em", bold = TRUE, color = "blue")

# Displaying the table
kable_table_isolation_case

# Define the path where you want 
#output_path_quarto <- here("results", "tables", "eee_isolation_cases")
output_path_png <- here("results", "tables", "eee_isolation_cases.png")


#save_kable(kable_table_isolation_case, file = output_path_quarto)
save_kable(kable_table_isolation_case, file = output_path_png, zoom = 4)

# Define the paths for the RDS files
first_positive_eee_test_by_year_path <- here("data", "processed-data", "rds", "first_positive_eee_test_by_year.rds")
human_cases_by_year_path <- here("data", "processed-data", "rds", "human_cases_by_year.rds")
animal_cases_by_year_path <- here("data", "processed-data", "rds", "animal_cases_by_year.rds")
combined_eee_data_path <- here("data", "processed-data", "rds", "combined_eee_data.rds")

# Save the data frames as RDS files
saveRDS(first_positive_eee_test_by_year, first_positive_eee_test_by_year_path)
saveRDS(human_cases_by_year, human_cases_by_year_path)
saveRDS(animal_cases_by_year, animal_cases_by_year_path)
saveRDS(combined_eee_data, combined_eee_data_path)
```

## Which county has the most EEE positive mosquitoes of those tested?

```{r}
eee_positive_by_county <- combined_virus_data %>%
  filter(Virus == "EEE") %>%  # Filter for rows where Virus is EEE
  group_by(County) %>%
  summarise(EEE_Positive_Tests = n()) %>%  # Count the number of EEE positive tests in each group
  arrange(desc(EEE_Positive_Tests))  # Order the results by EEE_Positive_Tests in descending order

# Create the table with kable and style it with kableExtra
eee_positive_table <- kable(eee_positive_by_county, "html", 
                            col.names = c("County", "EEE Positive Tests"),
                            caption = "EEE Positive Tests by County",
                            align = c('c','c')) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                full_width = FALSE, position = "center") %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(2, bold = TRUE, color = "red")

# Display the table
eee_positive_table

# Define the path where you want to save the HTML file
#output_path_qmd <- here("results", "tables", "eee_isolation_county.html")
output_path_county <- here("results", "tables", "eee_isolation_county.png")

# Save the kableExtra table as an HTML file
#save_kable(eee_positive_table, file = output_path_qmd)
save_kable(eee_positive_table, file = output_path_county, zoom = 6)
```

## *Culiseta melanura* (MEL) is the main enzootic vector of EEE. Some entomologist believe that high levels of MEL traps indicate higher risk for EEE transmission. Using the new mosquito collection table created from the pivot, let's calculate the proportion of MEL captured compared to all other mosquito vector species and create a plot to visualize it.

```{r}

# Define path to the data file
trap_wide_path <- here("data", "processed-data", "mosquito_wide_all.csv")

# Read the data from the file
trap_wide_data <- read_csv(trap_wide_path)

trap_wide_data <- trap_wide_data %>%
  mutate(Year = year(as.Date(`collection date`, format = "%Y-%m-%d")))
trap_wide_data
# Assuming all species columns are next to each other, calculate the proportion of MEL
proportion_mel_by_year <- trap_wide_data %>%
  rowwise() %>%
  mutate(Total_All = sum(c_across(ABS:OCH), na.rm = TRUE)) %>%
  ungroup() %>%
  group_by(Year) %>%
  summarise(
    Total_MEL = sum(MEL, na.rm = TRUE),
    Total_All = sum(Total_All, na.rm = TRUE), # Sum of all species by year
    MEL_Prop = Total_MEL / Total_All # Proportion of MEL
  )

# Print the results
print(proportion_mel_by_year)

# Calculate the proportion of MEL by week
# Assuming all species columns are next to each other, calculate the proportion of MEL by week_num
proportion_mel_by_week <- trap_wide_data %>%
  # Calculate row-wise totals for all species
  rowwise() %>%
  mutate(Total_All = sum(c_across(ABS:OCH), na.rm = TRUE)) %>%
  ungroup() %>%
  # Then group by week_num and summarize
  group_by(week_num) %>%
  summarise(Total_MEL = sum(MEL, na.rm = TRUE),
            Total_All = sum(Total_All, na.rm = TRUE), # Sum of all species by week_num
            MEL_Prop = Total_MEL / Total_All) # Proportion of MEL

# Print the results
print(proportion_mel_by_week)

# save the data as CSV
write_csv(proportion_mel_by_week, here("data", "processed-data", "mel_proportion_by_week.csv"))

# Calculate the average number of MEL by year
average_mel_by_year <- trap_wide_data %>%
  group_by(Year) %>%
  summarise(Average_MEL = mean(MEL, na.rm = TRUE))

# Save the processed data as RDS and CSV
saveRDS(proportion_mel_by_year, here("data", "processed-data", "rds", "yearly_mel_proportion.rds"))
write.csv(proportion_mel_by_year, here("data", "processed-data", "yearly_mel_proportion.csv"), row.names = FALSE)

saveRDS(average_mel_by_year, here("data", "processed-data", "rds", "yearly_mel_average.rds"))
write.csv(average_mel_by_year, here("data", "processed-data", "yearly_mel_average.csv"), row.names = FALSE)

# Plot for the proportion of MEL by year

mel_prop = ggplot(proportion_mel_by_year, aes(x = factor(Year), y = MEL_Prop)) +
  geom_col(fill = "steelblue") +
  labs(title = "Proportion of Culiseta melanura (MEL) by Year",
       x = "Year",
       y = "Proportion of MEL") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggsave(here("results", "figures", "mel_proportion_by_year.png"), width = 8, height = 6, dpi = 300)
mel_prop
 
# Plot for the average count of MEL by year
mel_ave = ggplot(average_mel_by_year, aes(x = factor(Year), y = Average_MEL, fill = "Year")) +
  geom_col() +
  labs(title = "Average Count of Culiseta melanura (MEL) by Year",
       x = "Year",
       y = "Average Count of MEL") +
  scale_fill_viridis_d() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

mel_ave

ggsave(here("results", "figures", "average_mel_by_year.png"), plot = mel_ave, width = 8, height = 6, dpi = 300)

```
## Average MEL by week
### IN addition to the average MEL by year, we will also look at the average MEL by week. This will give us a more granular view of the MEL population dynamics throughout the mosquito season.
#### This will be used in the analysis phase
```{r}
# Calculate the average number of MEL by week for all trap types
# Define path to the data file
trap_wide_path_all <- here("data", "processed-data", "mosquito_wide_all.csv")

# Read the data
trap_avg_week <- read_csv(trap_wide_path_all)

# Calculate the average number of MEL by week for all trap types
average_mel_week_all <- trap_avg_week %>%
  group_by(week_num, `trap type`) %>%
  summarise(Average_MEL = mean(MEL, na.rm = TRUE), .groups = 'drop')

# Examine the resulting data frame for all trap types
str(average_mel_week_all)

# Save the csv file with averages for all trap types
write_csv(average_mel_week_all, here("data", "processed-data", "average_mel_week_all.csv"))

# Now, filter data for specific CO2 baited light traps
trap_types_to_include <- c("500cc CO2 - CDC Miniature Light Trap", "Dry Ice CO2 - CDC Miniature Light Trap with CO2")
filtered_data <- trap_avg_week %>%
  filter(`trap type` %in% trap_types_to_include)

# Calculate the average number of MEL by week for the filtered CO2 baited traps
average_mel_week_co2 <- filtered_data %>%
  group_by(week_num, `trap type`) %>%
  summarise(Average_MEL = mean(MEL, na.rm = TRUE), .groups = 'drop')

# Examine the resulting data frame for CO2 baited traps
str(average_mel_week_co2)

# Save the csv file with averages for the CO2 baited traps
write_csv(average_mel_week_co2, here("data", "processed-data", "average_mel_week_co2.csv"))
```

## Mosquito abundance is affected by weather. Mosquitoes depend on standing water to complete their larval development. Let's take a look at the annual precipitation using the wx data frame.

```{r annual precip}
# Construct the path to the file
file_path_wx <- here("data", "processed-data", "wx_selected.csv")

# Import the data to R and store it in a dataframe named "wx"
wx <- read.csv(file_path_wx)

# Extract the year from each DATE and store it in a new column named "Year"
wx$Year <- as.integer(format(as.Date(wx$DATE), "%Y"))

# Now that we have the years as numeric values, let's filter for 2007 to 2020
filtered_wx <- wx %>%
  filter(Year >= 2007 & Year <= 2020)

# View the filtered dataframe
head(filtered_wx)



# Save the filtered weather data as an RDS file
filtered_wx_path_rds <- here("data", "processed-data", "rds", "filtered_wx_2007_2020.rds")
saveRDS(filtered_wx, filtered_wx_path_rds)

# Sum PRCP by Year for the filtered data
summed_prcp_by_year_filtered <- filtered_wx %>%
  group_by(Year) %>%
  summarise(Total_PRCP = sum(PRCP, na.rm = TRUE))

# Save the summarized precipitation data as an RDS file
summed_prcp_by_year_path_rds <- here("data", "processed-data", "rds", "summed_prcp_by_year_filtered_2013_2020.rds")
saveRDS(summed_prcp_by_year_filtered, summed_prcp_by_year_path_rds)

# View the result
print(summed_prcp_by_year_filtered)

# Create the ggplot object and assign it to a variable
precip_plot <- ggplot(summed_prcp_by_year_filtered, aes(x = as.factor(Year), y = Total_PRCP)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Total Precipitation by Year (2013-2020)",
       x = "Year",
       y = "Total Precipitation (inches)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

precip_plot

# Specify the file path where you want to save the plot
file_path_precip <- here("results", "figures", "annual_precip.png")

# Use ggsave to save the plot, make sure to use the plot object
ggsave(file_path_precip, plot = precip_plot, width = 8, height = 6, dpi = 300)
```

### Now I want to plot the ave mel by year with the total precip by year

```{r}
# Load the datasets
yearly_mel_ave <- readRDS(here("data", "processed-data", "rds", "yearly_mel_average.rds"))
summed_prcp_by_year_filtered <- readRDS(here("data", "processed-data", "rds", "summed_prcp_by_year_filtered_2013_2020.rds"))

# Combine the datasets
combined_data <- left_join(yearly_mel_ave, summed_prcp_by_year_filtered, by = "Year")

# Define the path for the RDS file with a descriptive name
combined_data_path_rds <- here("data", "processed-data", "rds", "combined_mel_avg_and_total_precip_2013_2020.rds")

# Save the combined data as an RDS file
saveRDS(combined_data, combined_data_path_rds)

plot_mel_precip <- ggplot(data = combined_data) +
  geom_col(aes(x = factor(Year), y = Average_MEL), fill = "blue") +
  geom_line(aes(x = factor(Year), y = Total_PRCP, group = 1), color = "red", linetype = "dashed", linewidth = 1) +
  geom_point(aes(x = factor(Year), y = Total_PRCP), color = "red", size = 3) +
  geom_hline(yintercept = 45.6, linetype = "dotted", color = "darkgreen", size = 1) +
  annotate("text", x = Inf, y = 45.6, label = "Avg Precip: 45.6 inches", 
           hjust = 3.2, vjust = -1.1, color = "darkgreen", size = 3.5, fontface = "bold", angle = 0) +
  scale_y_continuous(name = "Average Count of MEL", 
                     sec.axis = sec_axis(~ ., name = "Total Precipitation (inches)")) +
  labs(title = "Average Count of MEL Mosquitoes and Total Precipitation by Year",
       x = "Year") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.title.x = element_text(face = "bold"))

plot_mel_precip

# Save the plot
ggsave(filename = here("results", "figures", "combined_trends_mel_precip.png"), plot = last_plot(), width = 10, height = 6, dpi = 300)
```

### This will load sunset and sunrise times from the suncalc package. However, it can be slow to complete.
#### the amount of sunlight affects mosquitoes activities and provides cues for them to enter diapause.
##### The suncalc package will be used to calculate the sunrise and sunset times for Bristol County, MA, which is the study location. The times will be calculated for each day from 2007 to 2023. The data will be saved as a CSV file and an RDS file for future use.

```{r}
# Ensure the suncalc library is loaded
library(suncalc)
get_sun_times <- function(date, latitude, longitude) {
  # Get the sunrise and sunset times from the suncalc package
  times <- getSunlightTimes(date = date, lat = latitude, lon = longitude, tz = "UTC")
  
  # Convert sunrise and sunset times to POSIXct directly without checking class
  # Assume times are returned in a standard format that can be converted directly
  sunrise <- as.POSIXct(times$sunrise, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")
  sunset <- as.POSIXct(times$sunset, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")
  
  # Format sunrise and sunset times within the data frame creation step
  # Create a one-row data frame for each date
  data.frame(
    Date = as.character(date),
    Sunrise = format(sunrise, "%H:%M:%S", tz="UTC"),
    Sunset = format(sunset, "%H:%M:%S", tz="UTC")
  )
}

# Define the geographical coordinates for Bristol County, MA (latitude and longitude)
latitude <- 41.8744
longitude <- -71.0166

# Generate a sequence of dates from 2007-01-01 to 2023-12-31
dates <- seq(as.Date("2007-01-01"), as.Date("2023-12-31"), by = "day")

# Calculate sunrise and sunset times for each date
sun_times <- lapply(dates, function(date) {
  get_sun_times(date, latitude, longitude)
})

# Convert the list to a data frame
# this is a very large amount of data so it takes time to load. 
sun_times_df <- do.call(rbind, sun_times)
colnames(sun_times_df) <- c("Date", "Sunrise", "Sunset")

# Write the data frame to a CSV file
write.csv(sun_times_df, file = "sunrise_sunset_times_bristol_county_ma.csv", row.names = FALSE)

# Define the file path where you want to save the sunrise and sunset times data frame
sunrise_sunset_times_path <- here("data", "processed-data", "rds", "sunrise_sunset_times_bristol_county_ma.rds")

# Save the sun_times_df data frame as an RDS file
saveRDS(sun_times_df, sunrise_sunset_times_path)

# Output to confirm file saving
cat("Sunrise and sunset times saved to:", sunrise_sunset_times_path, "\n")


#here
# Add 'week_num' calculation here
sun_times_df <- sun_times_df %>%
  mutate(
    # Convert Date column to Date object if it's not already
    Date = as.Date(Date),
    Sunrise_dt = as.POSIXct(paste(Date, Sunrise), format="%Y-%m-%d %H:%M:%S", tz="UTC"),
    Sunset_dt = as.POSIXct(paste(Date, Sunset), format="%Y-%m-%d %H:%M:%S", tz="UTC"),
    # Calculate daylight hours
    daylight_hours = as.numeric(difftime(Sunset_dt, Sunrise_dt, units = "hours")),
    # Calculate week number as "YYYY-WW"
    week_num = paste(year(Date), sprintf("%02d", isoweek(Date)), sep = "-")
  )

# Now, proceed to aggregate by 'week_num'
average_daylight_by_week <- sun_times_df %>%
  group_by(week_num) %>%
  summarise(average_daylight_hours = mean(daylight_hours, na.rm = TRUE))  # Ensure NA values are handled

# Optionally, save or print your aggregated data
write.csv(average_daylight_by_week, here("data", "processed-data", "average_daylight_by_week.csv"), row.names = FALSE)
# Print the first few rows to verify
head(average_daylight_by_week)


# Define the path for the CSV file
avg_day_csv <- here("data", "processed-data", "average_daylight_by_week.csv")

# Write the data frame to a CSV file
write.csv(average_daylight_by_week, file = avg_day_csv, row.names = FALSE)

# Define the file path where you want to save the sunrise and sunset times data frame
average_daylight_by_week_path <- here("data", "processed-data", "rds", "average_daylight_by_week.rds")

# Save the sun_times_df data frame as an RDS file
saveRDS(sun_times_df, average_daylight_by_week_path)
```

