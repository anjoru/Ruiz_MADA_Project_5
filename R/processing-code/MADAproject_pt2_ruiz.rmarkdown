---
title: "MADA Project Part 2"
subtitle: "loading, cleaning, exploring the data"
author: "Andrew Ruiz"
date: 2024-02-23
format: html
editor: source
---


# Introduction

## This file contains the process for loading, cleaning and exploring the datasets.

### Use this code to ensure you have all the packages installed before proceeding.
#### This code will check if the packages are installed and install them if they are not.

```{r}
# List of packages required for the project
required_packages <- c(
  "readxl", "httr", "dplyr", "tidyr", "skimr", "here", "readr",
  "naniar", "ggplot2", "sf", "topicmodels", "openxlsx", "lubridate",
  "knitr", "kableExtra", "webshot2", "magick", "leaflet", "htmlwidgets",
  "suncalc", "stringr", "ISOweek"
)

# Function to check and install missing packages
install_missing_packages <- function(packages) {
  for (pkg in packages) {
    if (!require(pkg, character.only = TRUE, quietly = TRUE)) {
      install.packages(pkg)
      library(pkg, character.only = TRUE)
    }
  }
}

# Install missing packages
install_missing_packages(required_packages)
```


# Loading libraries
## This list contains the specific versions used in the script.

```{r, message=FALSE, warning=FALSE}
library(readxl) # for loading Excel files (version 1.4.0)
library(httr) # for getting data from the web (version 1.4.2)
library(dplyr) # for data processing/cleaning (version 1.0.7)
library(tidyr) # for data processing/cleaning (version 1.1.3)
library(skimr) # for visualizing data (version 2.1.3)
library(here) # allows relative pathways for data (version 1.0.1)
library(readr) # reading and writing data to and from CSV, TSV, and fixed-width files (version 2.0.1)
library(naniar) # tools to work with missing data (version 0.6.1)
library(ggplot2) # plot creation (version 3.3.5)
library(sf) # standardized way to encode spatial data (version 1.0-2)
library(topicmodels) # tools for topic modeling on text data (version 0.2-12)
library(openxlsx) # Facilitates reading from and writing to .xlsx files (version 4.2.4)
library(lubridate) # Simplifies the work with dates and times (version 1.7.10)
library(knitr) # Allows for dynamic report generation in R (version 1.33)
library(kableExtra) # generates simple HTML or LaTeX tables (version 1.3.4)
library(webshot2) # tools to take screenshots of web pages (version 0.0.2)
library(magick) # advanced image processing in R (version 2.7.3)
library(leaflet) # Enables the creation of interactive maps (version 2.0.4.1)
library(htmlwidgets) # Facilitates the creation of R bindings to JavaScript libraries (version 1.5.4)
library(suncalc) # Calculates sunset and sunrise times based on location (version 0.5.1)
library(stringr) # Makes string manipulation easier and more consistent (version 1.5.1)
library(ISOweek) # Calculates week number according to ISO 8601 standards (version 0.6-2)
library(purrr)
```


# Loading data
## Mosquito trap data
### This dataset contains trap counts by species for all mosquito traps set in the Bristol County from 2007-2023 this dataset will undergo extensive processing below.
#### The dataset was obtained though and open records request with the Commonwealth of MA.

```{r trap-data-import}
# Construct the path to the Excel file
#file_path_mos <- here("data", "raw-data", "MOSQ_MADA.xlsx")
file_path_mos <- here("data", "raw-data", "MOSQ_MADA.xlsx")
file_exists_check <- file.exists(file_path_mos)
file_path_mos <- here("data", "raw-data", "MOSQ_MADA.xlsx")

#file_exists_check <- file.exists("/Users/andrewruiz/MADA_course/RUIZ-MADA-project/data/raw-data/MOSQ_MADA.xlsx")
print(file_exists_check)  # Should return TRUE if the path is correct
print(file_path_mos)

print(file_exists_check)  # This should return TRUE if the file path is correct
# import the data to R and store it in a dataframe named "mosquito"
mosquito_raw <- read_excel(file_path_mos)

#load the first few rows of data to verify the data loaded as expected
head(mosquito_raw)
```


## Virus testing of the trap count data
### This dataset is identical to MOSQ_MADA except that it contains virus test results.

```{r}
# Read the dataset
pcr_data <- read_excel(here("data", "raw-data", "virus_iso.xlsx"))

# Add week number based on the collection data
# This code ensures that the ISO 8601 standard is applied to the week number calculation
# Week 1 of any year is the week that contains January 4th, or equivalently, it's the week that contains the first Thursday of January.
# Weeks start on Monday and end on Sunday.
# The last week of the year, week 52 or 53, is the one that contains December 28th.
# we will use week_num to join tables
# the format will be YYYY-WW. Week 8 of 2019 will look like: 2019-08
pcr_data <- pcr_data %>%
  mutate(
    # Directly format the collection date into "YYYY-WW"
    week_num = paste0(year(`Collection Date`), "-", 
                      sprintf("%02d", isoweek(`Collection Date`)))
  )

# Correctly expand "Both" in Test Type, then adjust to "EEE" and "WNV"
pcr_data_expanded <- pcr_data %>%
  mutate(Test_Type_Expanded = case_when(
    `Test Type` == "Both" ~ "EEE&WNV", # Temporarily mark "Both" for special handling
    TRUE ~ `Test Type`
  )) %>%
  separate_rows(Test_Type_Expanded, sep = "&") %>%
  mutate(
    Year = year(`Collection Date`),
    Week = week(`Collection Date`),
    Submitted_for_Testing = `Submitted for Testing` == "Y",
    # Assuming positive results are marked as "Positive" in `Result`
    Is_Positive = Result == "Positive"
  )

# Save as RDS
saveRDS(pcr_data_expanded, here("data", "processed-data", "rds", "pcr_data_expanded.rds"))

# Save as CSV
write_csv(pcr_data_expanded, here("data", "processed-data", "pcr_data_expanded.csv"))

```


## Mosquito trap sites
### Location for the mosquito trap sites

```{r}
# Construct the path to the file
file_path_trap = here("data", "processed-data", "BCMCP_trap_sites_2021.xlsx")

# import the data to R and store it in a dataframe named "trap_location"
trap_location = read_excel(file_path_trap)

#load the first few rows of data to verify the data loaded as expected
head(trap_location)

#The last 3 columns have missing data. However, they are not important for the purposes of this project
# there is not a need to delete the records
# this is a free text field that provides notes about the location.
# they are not needed so we will delete them
trap_location_selected <- trap_location %>%
  select(`Trap Site Address`, MCD, Location, DecLat, DecLong)
trap_location_selected

# Define the path for the trap locations selected dataset RDS
trap_location_selected_path_rds <- here("data", "processed-data", "rds", "trap_sites_selected.rds")

# Save the trap_location_selected dataframe as an RDS file
saveRDS(trap_location_selected, trap_location_selected_path_rds)
```


## Mosquito virus data

### The data includes all the positive PCR results for EEE and WNV for all mosquitoes submitted for testing

### These files were obtained from: https://www.mass.gov/lists/arbovirus-surveillance-plan-and-historical-data

### The virus isolation data is stored in separate spreadsheets by year from 2014 to 2020

### This code will load all the files, combine them, and then save the combined file in the "raw data" folder


```{r}
#Define the path to the folder with Excel files
folder_path <- here("data", "processed-data", "virus_data")

#List all Excel files (.xls and .xlsx)
file_paths <- list.files(path = folder_path, pattern = "\\.xls[x]*$", full.names = TRUE)

#Read each file into a list of data frames
data_list <- lapply(file_paths, read_excel)

#Check the names of the files read
print(basename(file_paths))

#Combine the datasets
combined_virus_data <- bind_rows(data_list)

# Define the path for the combined dataset
combined_path <- here("data", "processed-data", "combined_virus_data.csv")

# Save the combined data frame as a CSV file
write.csv(combined_virus_data, combined_path, row.names = FALSE)

# Define the path for the combined dataset RDS
combined_path_rds <- here("data", "processed-data", "rds", "combined_virus_data.rds")

# Save the combined data frame as an RDS file
saveRDS(combined_virus_data, combined_path_rds)

# View the first few rows to confirm it's loaded correctly
head(combined_virus_data)
```



## Weather data
### temperature and precipitation are important factors in mosquito population dynamics
### This dataset contains daily weather summaries from the Taunton, MA Airport from 2007-2023
#### The data was obtained from the NOAA National Centers for Environmental Information

```{r}
# Construct the path to the file
file_path_wx = here("data", "processed-data", "weather_airport.csv")

# import the data to R and store it in a dataframe named "wx"
wx = read.csv(file_path_wx)

wx <- wx %>%
  mutate(
    DATE = mdy(DATE),  # Parse the DATE column from "month/day/year"
    week_num = paste0(year(DATE), "-", sprintf("%02d", isoweek(DATE)))  # Create week_num
  )
#load the first few rows of data to verify the data loaded as expected
head(wx)

#the columns below are the only ones needed for this project. 
wx_selected <- wx %>%
  dplyr::select(STATION, NAME, DATE, PRCP, TMAX, week_num)

# Define the path for the RDS file
output_path_wx <- here("data", "processed-data", "rds", "wx_selected.rds")
output_csv_wx <- here("data", "processed-data", "wx_selected.csv")
# Save the wx_selected dataframe as an RDS file
saveRDS(wx_selected, output_path_wx)
write.csv(wx_selected, output_csv_wx, row.names = FALSE)
head(wx_selected)
```

## Drought data
### These datasets were obtained from the US Drought Monitor
#### The U.S. Drought Monitor is produced through a partnership between the National Drought Mitigation Center at the University of Nebraska-Lincoln, the United States Department of Agriculture and the National Oceanic and Atmospheric Administration
##### This data is organized by geographic area and week
##### It includes 4 categories. D4 - Exceptional Drought (0-2 percentile), D3 - Extreme Drought (2-5 percentile), D2 - Severe Drought (5-10 percentile), D1 - Moderate Drought (10-20 percentile), D0 - Abnormally Dry (20-30 percentile), None or 0 - Neither Wet or Dry
##### It provides the percent area covered by each category. 

```{r}
# read file with drought data by county for MA, RI, CT for the years 2007-2023
drought_ma_ri_ct = read_csv(here("data", "raw-data", "comp_stat_county_ma_ri_ct.csv"), show_col_types = FALSE)

# read file with drought data by county for FEMA Region 4 for the years 2007-2023
drought_fema4 = read_csv(here("data", "raw-data", "comp_stat_fema4.csv"), show_col_types = FALSE)

# Add week number based on MapDate for both files
# This code ensures that the ISO 8601 standard is applied to the week number calculation
# Week 1 of any year is the week that contains January 4th, or equivalently, it's the week that contains the first Thursday of January.
# Weeks start on Monday and end on Sunday.
# The last week of the year, week 52 or 53, is the one that contains December 28th.
# we will use week_num to join tables
# the format will be YYYY-WW. Week 8 of 2019 will look like: 2019-08
drought_ma_ri_ct <- drought_ma_ri_ct %>%
  mutate(
    # First, ensure MapDate is in a proper date format
    MapDate = as.Date(as.character(MapDate), format = "%Y%m%d"),
    # Then, format the collection date into "YYYY-WW"
    week_num = paste0(year(MapDate), "-", 
                      sprintf("%02d", isoweek(MapDate)))
  )

drought_fema4 <- drought_fema4 %>%
  mutate(
    # First, ensure MapDate is in a proper date format
    MapDate = as.Date(as.character(MapDate), format = "%Y%m%d"),
    # Then, format the collection date into "YYYY-WW"
    week_num = paste0(year(MapDate), "-", 
                      sprintf("%02d", isoweek(MapDate)))
  )

# Define the path for the RDS and CSV files
rds_path_drought_ma_ri_ct<- here("data", "processed-data", "rds", "drought_ma_ri_ct.rds")
csv_path_drought_ma_ri_ct <- here("data", "processed-data", "drought_ma_ri_ct.csv")
# Save the wx_selected dataframe as an RDS and CSV files
saveRDS(drought_ma_ri_ct, rds_path_drought_ma_ri_ct)
write.csv(drought_ma_ri_ct, csv_path_drought_ma_ri_ct, row.names = FALSE)

# Define the path for the RDS and CSV files
rds_path_drought_fema4<- here("data", "processed-data", "rds", "drought_fema4.rds")
csv_path_drought_fema4 <- here("data", "processed-data", "drought_fema4.csv")
# Save the wx_selected dataframe as an RDS and CSV files
saveRDS(drought_fema4, rds_path_drought_fema4)
write.csv(drought_fema4, csv_path_drought_fema4, row.names = FALSE)
```


## Human and veterinary cases of eastern equine encephalitis and West Nile virus

### these were extracted from annual mosquito reports publised here: https://www.mass.gov/lists/arbovirus-surveillance-plan-and-historical-data

### human cases are listed by county only. animal cases are listed by town/city (with the exception of 1 year)


```{r}
# Construct the path to the human case data file
file_path_human <- here("data", "processed-data", "human_cases.xlsx")

# import the data to R and store it in a dataframe named "cases_human"
cases_human <- read_excel(file_path_human)

# Save 'cases_human' as an RDS file
output_path_rds_human <- here("data", "processed-data", "rds", "human_cases_processed.rds")
saveRDS(cases_human, output_path_rds_human)

#load the first few rows of data to verify the data loaded as expected
head(cases_human)

# Construct the path to the veterinary case data file
file_path_vet <- here("data", "processed-data", "animal_cases.xlsx")

# import the data to R and store it in a dataframe named "cases_human"
cases_vet <- read_excel(file_path_vet)

# Save 'cases_vet' as an RDS file
output_path_rds_vet <- here("data", "processed-data", "rds", "vet_cases_processed.rds")
saveRDS(cases_vet, output_path_rds_vet)

#load the first few rows of data to verify the data loaded as expected
head(cases_vet)
#date of onset is missing for some records, however, they are not needed for this project. 
```


# Processing and cleaning the Mosquito trap data

### Examine the variables and change all column names to lower case


```{r}
str(mosquito_raw)

# Set column names to lowercase
names(mosquito_raw) <- tolower(names(mosquito_raw))

# Check the first few rows to confirm changes
head(mosquito_raw)
```


## we will examine the dataset before making changes.


### first we will Identify missing data


```{r}
sapply(mosquito_raw, function(x) sum(is.na(x)))
```


#### The majority of the missing data is in the "Submitted for Testing" variable. This is not a crucial variable, so we can keep those records. Missing data in that variable generally means 0 mosquitoes were captured in a trap. The other variable are important and so we will remove those records with the other missing data.

### The Massachussetts Department of Public Health adpoted a taxonomy change before it was fully vetted. The change divided the genus *Aedes* into a new genus: *Ochlerotatus*. To align with accepted standards, I will change them back to *Aedes*.


```{r}
# Get unique values using dplyr
unique_genus_values <- mosquito_raw %>%
  distinct(genus) %>%
  pull(genus)

# Print the unique genus values
print(unique_genus_values)

# Change records where genus is "Ochlerotatus" to "Aedes"
mosquito_raw$genus[mosquito_raw$genus == "Ochlerotatus"] <- "Aedes"

# Check the first few rows to confirm changes, or use unique() to see the change in genus values
unique(mosquito_raw$genus)
```


### Execute proposed changes.


```{r}
# Create a copy of the dataframe to avoid modifying the original
mosquito_modified <- mosquito_raw

# Change records where genus is "Ochlerotatus" to "Aedes" to adhere to current taxonomy standards
mosquito_modified$genus[mosquito_modified$genus == "Ochlerotatus"] <- "Aedes"

# Ensure Collection Date is in Date format 
mosquito_modified$`collection date` <- as.Date(mosquito_modified$`collection date`)

# Convert all species code values to uppercase
# This will minimize issues were data as entered incorrectly
# for example Mel, mel, and MEL
mosquito_modified <- mosquito_modified %>%
  mutate(`species code` = toupper(`species code`))

# Remove rows with NA values in the remaining columns of the modified dataframe, 
# except for "Submitted for Testing" which we'll handle separately
# when Submitted for testing is NA or missing, it indicates that there were no mosquitoes captured during a trap event.
# This is common at the beginning and end of season and also when a trap malfunctions
temp_columns <- names(mosquito_modified)
temp_columns <- temp_columns[temp_columns != "submitted for testing"]
mosquito_modified <- na.omit(mosquito_modified, cols = temp_columns)

# Add "week_num" column
# This code ensures that the ISO 8601 standard is applied to the week number calculation
# Week 1 of any year is the week that contains January 4th, or equivalently, it's the week that contains the first Thursday of January.
# Weeks start on Monday and end on Sunday.
# The last week of the year, week 52 or 53, is the one that contains December 28th.
# we will use week_num to join tables
# the format will be YYYY-WW. Week 8 of 2019 will look like: 2019-08
mosquito_modified <- mosquito_modified %>%
  mutate(
    week_num = paste0(year(`collection date`), "-", sprintf("%02d", isoweek(`collection date`)))
  )
# Check the structure of the final cleaned dataframe
str(mosquito_modified)

# before filtering the dates to have a cleaned dataset with all the records.
write_csv(mosquito_modified, here("data", "processed-data", "mosquito_clean_all.csv"))
saveRDS(mosquito_modified, here("data", "processed-data", "rds", "mosquito_clean_all.rds"))
```


## Reformat the collection data

### The Massachusetts Department of Public Health and US CDC require that all mosquito control districts submit their colelction data in a specific format. This format divides each trap event into separate rows based on mosquito species. However, this format is not useful for certain calculations where a zero count for a species in not explicitly recorded. Even if you sort by species and calculate average count per trap, the average could be inflated since the records will not include trap events with a zero count of that species.

### In order to correct this, I will pivot the data and create a column for every unique "species code" and sum the "pool size" for each species code. The resulting table will have one row for every trap event -a trap event is when "town", "date of collection", "trap type" are all the same.

### The resulting data frame will rectangle in shape and contain 1 row per trap event with all possible species as a separate column.



```{r}
library(dplyr)
library(tidyr)
library(purrr)
library(lubridate)

# Step 1: Summarize the data
mosquito_summarized <- mosquito_modified %>%
  group_by(`collection date`, town, `trap type`, `species code`, mcd, `submitted for testing`) %>%
  summarise(pool_size_sum = sum(`pool size`, na.rm = TRUE), .groups = 'drop')

# Step 2: Pivot to wide format
wide_data <- mosquito_summarized %>%
  pivot_wider(
    names_from = `species code`,
    values_from = pool_size_sum,
    id_cols = c(`collection date`, town, `trap type`, mcd)
  )

# Define species_columns after pivoting
species_columns <- setdiff(names(wide_data), c("collection date", "town", "trap type", "mcd"))

# Ensure there's no duplication in the conversion attempt
# Step 3: Convert list columns to numeric, replacing NULL or list elements without values with 0
# Utilizing sapply for a direct approach to handle potential list-columns
wide_data <- wide_data %>%
  mutate(across(.cols = all_of(species_columns), 
                .fns = ~ map_dbl(.x, function(item) {
                  # Handle NULL items directly
                  if (is.null(item)) {
                    return(0)
                  # Handle vectors: convert NA to 0, and take the first item if it's a vector
                  } else if (length(item) > 1 || is.na(item)) {
                    item[is.na(item)] <- 0
                    return(as.numeric(item[1]))
                  } else {
                    return(as.numeric(item))
                  }
                })))
# Step 4: Add week_num column
wide_data <- wide_data %>%
  mutate(
    `collection date` = as.Date(`collection date`),
    week_num = paste0(year(`collection date`), "-", sprintf("%02d", isoweek(`collection date`)))
  )

# Output the transformed data to inspect its structure
head(wide_data)

# Save the new file
# Specify the file path using here()
file_path_wide <- here("data", "processed-data", "mosquito_wide_all.csv")

# Save the wide-format data to the specified path
write_csv(wide_data, file_path_wide)

# Define the path for the wide-format dataset RDS
file_path_wide_rds <- here("data", "processed-data", "rds", "mosquito_wide_all.rds")

# Save the wide-format data as an RDS file
saveRDS(wide_data, file_path_wide_rds)

# Now we will filter the the mosquito data to match the time period of the virus data. 
# Since mosquito populations can be affected by the previous year's populations, we will filter for 2011-

# Filter wide_data for the specified date range
filtered_mosquito_data <- wide_data %>%
  filter(`collection date` >= as.Date("2007-01-01") & `collection date` <= as.Date("2023-12-31"))

# Define the path for the CSV file
file_path_filtered_csv <- here("data", "processed-data", "mosquito_wide_filtered.csv")

# Save the filtered wide-format data to the specified CSV path
write_csv(filtered_mosquito_data, file_path_filtered_csv)

# Define the path for the RDS file
file_path_filtered_rds <- here("data", "processed-data", "rds", "mosquito_wide_filtered.rds")

# Save the filtered wide-format data as an RDS file
saveRDS(filtered_mosquito_data, file_path_filtered_rds)

```



# Exploring the Data

##Study location Below is a map of the mosquito trap sites. This is an interactive map and will only work on web or other html-friendly platforms.


```{r}
# Create the map
map_study <- leaflet(trap_location) %>%
  addTiles() %>%  # This adds the default OpenStreetMap tiles
  addMarkers(lng = ~DecLong, lat = ~DecLat, popup = ~Location)  # Customize the popup content as needed

# Display the map
map_study

# Specify the path and name of your HTML file
file_path_map <- here("results", "figures", "map_study.html")

# Save the map
saveWidget(map_study, file_path_map, selfcontained = TRUE)
```


## EEE in mosquitoes and mammals

### To give an idea of when EEE virus spilled over from mosquitoes and birds and into mammal populations, lets take a look at the data. Let's see when the first EEE isolations were found in mosquitoes each year. Let's also look at the years that human and veterinary cases occured.

### From the table, we see that human or othe mammal cases were recorded in 2014, 2018, 2019, and 2020. 2019 had the highest case counts for both humans and other mammals. **Because of this, we will pay close attention to 2019 through out the exploration.**


```{r}

# virus_data, cases_human, and cases_vet data frames 
# Prepare virus_data: Find the first positive EEE test date by year
first_positive_eee_test_by_year <- combined_virus_data %>%
  filter(Virus == "EEE") %>%
  mutate(Year = year(as.Date(`Collection Date`))) %>%
  group_by(Year) %>%
  summarise(First_Positive_Test_Date = min(`Collection Date`)) %>%
  ungroup() %>%
  arrange(Year)

# Prepare cases_human: Summarize human EEE cases by year
human_cases_by_year <- cases_human %>%
  filter(`Virus Result` == "EEE") %>%
  mutate(Year = year(`Onset Date`)) %>%
  group_by(Year) %>%
  summarise(Human_Cases = n()) %>%
  ungroup() %>%
  arrange(Year)

# Prepare cases_vet: Summarize animal EEE cases by year
animal_cases_by_year <- cases_vet %>%
  filter(virus == "eee") %>%
  group_by(year) %>%
  summarise(Animal_Cases = n()) %>%
  ungroup() %>%
  arrange(year)

# Ensure 'Year' column is numeric across all data frames for compatibility
# Note: This step might already be covered by the mutations above but included here for clarity
first_positive_eee_test_by_year$Year <- as.numeric(first_positive_eee_test_by_year$Year)
animal_cases_by_year$year <- as.numeric(animal_cases_by_year$year) # Make sure year is numeric and named consistently

# Merge the summaries into one combined data frame
combined_eee_data <- first_positive_eee_test_by_year %>%
  left_join(human_cases_by_year, by = "Year") %>%
  left_join(animal_cases_by_year, by = c("Year" = "year")) # Ensure correct column names are used for joining

# Replace NA values with 0 for Human_Cases and Animal_Cases
combined_eee_data$Human_Cases[is.na(combined_eee_data$Human_Cases)] <- 0
combined_eee_data$Animal_Cases[is.na(combined_eee_data$Animal_Cases)] <- 0

# Creating the table with centered headings and data
kable_table_isolation_case <- kable(combined_eee_data, "html", 
                     col.names = c("Year", "First Positive EEE Test Date", "Human EEE Cases", "Animal EEE Cases"),
                     caption = "Annual Summary of EEE Virus Activity",
                     align = c('c','c','c','c')) %>% # This aligns all columns' data to center
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                full_width = F, position = "center") %>%
  add_header_above(c(" " = 1, "EEE Virus Detection and Cases" = 3)) %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(2, width = "8em") %>% # Adjusted width without altering alignment
  column_spec(3, width = "5em", bold = TRUE, color = "red") %>%
  column_spec(4, width = "5em", bold = TRUE, color = "blue")

# Displaying the table
kable_table_isolation_case

# Define the path where you want 
#output_path_quarto <- here("results", "tables", "eee_isolation_cases")
output_path_png <- here("results", "tables", "eee_isolation_cases.png")


#save_kable(kable_table_isolation_case, file = output_path_quarto)
save_kable(kable_table_isolation_case, file = output_path_png, zoom = 2)

# Define the paths for the RDS files
first_positive_eee_test_by_year_path <- here("data", "processed-data", "rds", "first_positive_eee_test_by_year.rds")
human_cases_by_year_path <- here("data", "processed-data", "rds", "human_cases_by_year.rds")
animal_cases_by_year_path <- here("data", "processed-data", "rds", "animal_cases_by_year.rds")
combined_eee_data_path <- here("data", "processed-data", "rds", "combined_eee_data.rds")

# Save the data frames as RDS files
saveRDS(first_positive_eee_test_by_year, first_positive_eee_test_by_year_path)
saveRDS(human_cases_by_year, human_cases_by_year_path)
saveRDS(animal_cases_by_year, animal_cases_by_year_path)
saveRDS(combined_eee_data, combined_eee_data_path)
```


## Which county has the most EEE positive mosquitoes of those tested?


```{r}
eee_positive_by_county <- combined_virus_data %>%
  filter(Virus == "EEE") %>%  # Filter for rows where Virus is EEE
  group_by(County) %>%
  summarise(EEE_Positive_Tests = n()) %>%  # Count the number of EEE positive tests in each group
  arrange(desc(EEE_Positive_Tests))  # Order the results by EEE_Positive_Tests in descending order

# Create the table with kable and style it with kableExtra
eee_positive_table <- kable(eee_positive_by_county, "html", 
                            col.names = c("County", "EEE Positive Tests"),
                            caption = "EEE Positive Tests by County",
                            align = c('c','c')) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                full_width = FALSE, position = "center") %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(2, bold = TRUE, color = "red")

# Display the table
eee_positive_table

# Define the path where you want to save the HTML file
#output_path_qmd <- here("results", "tables", "eee_isolation_county.html")
output_path_county <- here("results", "tables", "eee_isolation_county.png")

# Save the kableExtra table as an HTML file
#save_kable(eee_positive_table, file = output_path_qmd)
save_kable(eee_positive_table, file = output_path_county, zoom = 4)
```


## *Culiseta melanura* (MEL) is the main enzootic vector of EEE. Some entomologist believe that high levels of MEL traps indicate higher risk for EEE transmission. Using the new mosquito collection table created from the pivot, let's calculate the proportion of MEL captured compared to all other mosquito vector species and create a plot to visualize it.


```{r}

# Define path to the data file
trap_wide_path <- here("data", "processed-data", "mosquito_wide.csv")

# Read the data
trap_proportion <- read_csv(trap_wide_path) %>%
  mutate(Year = year(as.Date(`collection date`, format = "%Y-%m-%d")))

# Calculate the proportion of MEL out of the total for all species
proportion_mel_by_year <- trap_proportion %>%
  group_by(Year) %>%
  summarise(Total_MEL = sum(MEL, na.rm = TRUE),
            Total_All = sum(MEL + PER + PIP + RES + SAL + TRI + SAP, na.rm = TRUE),
            MEL_Prop = Total_MEL / Total_All)

# Calculate the average number of MEL by year
average_mel_by_year <- trap_proportion %>%
  group_by(Year) %>%
  summarise(Average_MEL = mean(MEL, na.rm = TRUE))

# Save the processed data as RDS and CSV
saveRDS(proportion_mel_by_year, here("data", "processed-data", "rds", "yearly_mel_proportion.rds"))
write.csv(proportion_mel_by_year, here("data", "processed-data", "yearly_mel_proportion.csv"), row.names = FALSE)

saveRDS(average_mel_by_year, here("data", "processed-data", "rds", "yearly_mel_average.rds"))
write.csv(average_mel_by_year, here("data", "processed-data", "yearly_mel_average.csv"), row.names = FALSE)

# Plot for the proportion of MEL by year

mel_prop = ggplot(proportion_mel_by_year, aes(x = factor(Year), y = MEL_Prop)) +
  geom_col(fill = "steelblue") +
  labs(title = "Proportion of Culiseta melanura (MEL) by Year",
       x = "Year",
       y = "Proportion of MEL") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggsave(here("results", "figures", "mel_proportion_by_year.png"), width = 8, height = 6, dpi = 300)
mel_prop
 
# Plot for the average count of MEL by year
mel_ave = ggplot(average_mel_by_year, aes(x = factor(Year), y = Average_MEL, fill = "Year")) +
  geom_col() +
  labs(title = "Average Count of Culiseta melanura (MEL) by Year",
       x = "Year",
       y = "Average Count of MEL") +
  scale_fill_viridis_d() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

mel_ave

ggsave(here("results", "figures", "average_mel_by_year.png"), plot = mel_ave, width = 8, height = 6, dpi = 300)

```

## Average MEL by week
### IN addition to the average MEL by year, we will also look at the average MEL by week. This will give us a more granular view of the MEL population dynamics throughout the mosquito season.
#### This will be used in the analysis phase

```{r}
# Calculate the average number of MEL by week
# Define path to the data file
trap_wide_path_all <- here("data", "processed-data", "mosquito_wide_all.csv")

# Read the data
trap_avg_week <- read_csv(trap_wide_path_all)
average_mel_week <- trap_avg_week %>%
  group_by(week_num, `trap type`) %>%
  summarise(Average_MEL = mean(MEL, na.rm = TRUE))
str(average_mel_week)
 # save the csv file
write_csv(average_mel_week, here("data", "processed-data", "average_mel_week.csv"))

# Read the data
trap_wide_path_all <- here("data", "processed-data", "mosquito_wide_all.csv")
trap_avg_week <- read_csv(trap_wide_path_all)

# Filter data for specific trap types
## CO2 baited light traps.
### these are the most effective traps for capturing MEL
trap_types_to_include <- c("500cc CO2 - CDC Miniature Light Trap", "Dry Ice CO2 - CDC Miniature Light Trap with CO2")
filtered_data <- trap_avg_week %>%
  filter(`trap type` %in% trap_types_to_include)

# Calculate the average number of MEL by week for the filtered data
average_mel_weekco2 <- filtered_data %>%
  group_by(week_num) %>%
  summarise(Average_MEL = mean(MEL, na.rm = TRUE))

# examine the resulting data frame
str(average_mel_weekco2)
# save the csv file
write_csv(average_mel_weekco2, here("data", "processed-data", "average_mel_weekco2.csv"))

```


## Mosquito abundance is affected by weather. Mosquitoes depend on standing water to complete their larval development. Let's take a look at the annual precipitation using the wx data frame.


```{r annual precip}
# Convert DATE from character to Date type using as.Date and specifying the format
wx$DATE <- as.Date(wx$DATE, format="%m/%d/%y")

# The as.Date function treats years in the format "yy" as follows:
# Years 00-68 are treated as 2000-2068, and years 69-99 are treated as 1969-1999.
# Adjust this according to your data's actual year range if needed.

# Extract the year from each DATE
wx$Year <- format(wx$DATE, "%Y") %>% as.numeric()

# Now that we have the years as numeric values, let's filter for 2007 to 2020
filtered_wx <- wx %>%
  filter(Year >= 2007 & Year <= 2020)

# Save the filtered weather data as an RDS file
filtered_wx_path_rds <- here("data", "processed-data", "rds", "filtered_wx_2007_2020.rds")
saveRDS(filtered_wx, filtered_wx_path_rds)

# Sum PRCP by Year for the filtered data
summed_prcp_by_year_filtered <- filtered_wx %>%
  group_by(Year) %>%
  summarise(Total_PRCP = sum(PRCP, na.rm = TRUE))

# Save the summarized precipitation data as an RDS file
summed_prcp_by_year_path_rds <- here("data", "processed-data", "rds", "summed_prcp_by_year_filtered_2013_2020.rds")
saveRDS(summed_prcp_by_year_filtered, summed_prcp_by_year_path_rds)

# View the result
print(summed_prcp_by_year_filtered)

# Create the ggplot object and assign it to a variable
precip_plot <- ggplot(summed_prcp_by_year_filtered, aes(x = as.factor(Year), y = Total_PRCP)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Total Precipitation by Year (2013-2020)",
       x = "Year",
       y = "Total Precipitation (inches)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

precip_plot

# Specify the file path where you want to save the plot
file_path_precip <- here("results", "figures", "annual_precip.png")

# Use ggsave to save the plot, make sure to use the plot object
ggsave(file_path_precip, plot = precip_plot, width = 8, height = 6, dpi = 300)
```


### Now I want to plot the ave mel by year with the total precip by year


```{r}
# Load the datasets
yearly_mel_ave <- readRDS(here("data", "processed-data", "rds", "yearly_mel_ave.rds"))
summed_prcp_by_year_filtered <- readRDS(here("data", "processed-data", "rds", "summed_prcp_by_year_filtered_2013_2020.rds"))

# Combine the datasets
combined_data <- left_join(yearly_mel_ave, summed_prcp_by_year_filtered, by = "Year")

# Define the path for the RDS file with a descriptive name
combined_data_path_rds <- here("data", "processed-data", "rds", "combined_mel_avg_and_total_precip_2013_2020.rds")

# Save the combined data as an RDS file
saveRDS(combined_data, combined_data_path_rds)


plot_mel_precip = ggplot(data = combined_data) +
  geom_col(aes(x = factor(Year), y = Average_MEL), fill = "blue") +
  geom_line(aes(x = factor(Year), y = Total_PRCP, group = 1), color = "red", linetype = "dashed") +
  geom_point(aes(x = factor(Year), y = Total_PRCP), color = "red", size = 3) +
  geom_hline(yintercept = 45.6, linetype = "dotted", color = "darkgreen", size = 1) +
  annotate("text", x = Inf, y = 45.6, label = "Avg Precip: 45.6 inches", 
           hjust = 3.2, vjust = -1.1, color = "darkgreen", size = 3.5, fontface = "bold", angle = 0) +
  scale_y_continuous(name = "Average Count of MEL", 
                     sec.axis = sec_axis(~ ., name = "Total Precipitation (inches)")) +
  labs(title = "Average Count of MEL Mosquitoes and Total Precipitation by Year",
       x = "Year") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.title.x = element_text(face = "bold"))
plot_mel_precip

# Save the plot
ggsave(filename = here("results", "figures", "combined_trends_mel_precip.png"), plot = last_plot(), width = 10, height = 6, dpi = 300)
```


### This will load sunset and sunrise times.
#### the amount of sunlight affects mosquitoes activities and provides cues for them to enter diapause.
##### The suncalc package will be used to calculate the sunrise and sunset times for Bristol County, MA, which is the study location. The times will be calculated for each day from 2007 to 2023. The data will be saved as a CSV file and an RDS file for future use.

```{r}
# Ensure the suncalc library is loaded
library(suncalc)
get_sun_times <- function(date, latitude, longitude) {
  # Get the sunrise and sunset times from the suncalc package
  times <- getSunlightTimes(date = date, lat = latitude, lon = longitude, tz = "UTC")
  
  # Convert sunrise and sunset times to POSIXct directly without checking class
  # Assume times are returned in a standard format that can be converted directly
  sunrise <- as.POSIXct(times$sunrise, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")
  sunset <- as.POSIXct(times$sunset, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")
  
  # Format sunrise and sunset times within the data frame creation step
  # Create a one-row data frame for each date
  data.frame(
    Date = as.character(date),
    Sunrise = format(sunrise, "%H:%M:%S", tz="UTC"),
    Sunset = format(sunset, "%H:%M:%S", tz="UTC")
  )
}

# Define the geographical coordinates for Bristol County, MA (latitude and longitude)
latitude <- 41.8744
longitude <- -71.0166

# Generate a sequence of dates from 2007-01-01 to 2023-12-31
dates <- seq(as.Date("2007-01-01"), as.Date("2023-12-31"), by = "day")

# Calculate sunrise and sunset times for each date
sun_times <- lapply(dates, function(date) {
  get_sun_times(date, latitude, longitude)
})

# Convert the list to a data frame
# this is a very large amount of data so it takes time to load. 
sun_times_df <- do.call(rbind, sun_times)
colnames(sun_times_df) <- c("Date", "Sunrise", "Sunset")

# Write the data frame to a CSV file
write.csv(sun_times_df, file = "sunrise_sunset_times_bristol_county_ma.csv", row.names = FALSE)

# Define the file path where you want to save the sunrise and sunset times data frame
sunrise_sunset_times_path <- here("data", "processed-data", "rds", "sunrise_sunset_times_bristol_county_ma.rds")

# Save the sun_times_df data frame as an RDS file
saveRDS(sun_times_df, sunrise_sunset_times_path)

# Output to confirm file saving
cat("Sunrise and sunset times saved to:", sunrise_sunset_times_path, "\n")

#Average daylight by week_num
sun_times_df <- sun_times_df %>%
  mutate(
    # Combine Date with Sunrise and Sunset times to create full POSIXct datetime objects
    Sunrise_dt = as.POSIXct(paste(Date, Sunrise), format="%Y-%m-%d %H:%M:%S", tz="UTC"),
    Sunset_dt = as.POSIXct(paste(Date, Sunset), format="%Y-%m-%d %H:%M:%S", tz="UTC"),
    # Now calculate the difference in hours between Sunset and Sunrise
    daylight_hours = as.numeric(difftime(Sunset_dt, Sunrise_dt, units = "hours"))
  )

# Aggregate data by week and calculate average daylight hours
average_daylight_by_week <- sun_times_df %>%
  group_by(week_num) %>%
  summarise(average_daylight_hours = mean(daylight_hours))

# Print first few rows of the resulting data frame
head(average_daylight_by_week)

# Define the path for the CSV file
avg_day_csv <- here("data", "processed-data", "average_daylight_by_week.csv")

# Write the data frame to a CSV file
write.csv(average_daylight_by_week, file = avg_day_csv, row.names = FALSE)

# Define the file path where you want to save the sunrise and sunset times data frame
average_daylight_by_week_path <- here("data", "processed-data", "rds", "average_daylight_by_week.rds")

# Save the sun_times_df data frame as an RDS file
saveRDS(sun_times_df, average_daylight_by_week_path)
```


### Calcualte the average moon phase by week_num.
#### some resaecrh suggests that moon phase can affect mosquito activity.
##### The moon phase is calculated using the getMoonIllumination function from the lunar package. The function returns the percentage of the moon's illuminated surface area and the moon's phase angle. The phase angle is used to determine the moon phase, which can be one of the following: "New Moon", "First Quarter", "Full Moon", or "Last Quarter". The phase angle is converted to a value between 0 and 1, where 0 corresponds to a New Moon, 0.5 corresponds to a Full Moon, and 1 corresponds to a New Moon. The average moon phase for each week is calculated by taking the mean of the moon phase values for each day in the week.


```{r}
# Generate a sequence of dates from 2007-01-01 to 2023-12-31
dates <- seq(as.Date("2007-01-01"), as.Date("2023-12-31"), by = "day")

# Function to get moon phase for a given date
get_moon_phase <- function(date) {
  illumination <- getMoonIllumination(date)
  phase <- illumination$phase
  return(phase)
}

# Calculate moon phase for each date
moon_phases <- sapply(dates, get_moon_phase)

# Create a data frame with dates and their corresponding moon phases
moon_phases_df <- data.frame(Date = dates, MoonPhase = moon_phases)

# View the first few rows of the data frame
str(moon_phases_df)

# Add week_num column to moon_phases_df
moon_phases_df <- moon_phases_df %>%
  mutate(
    week_num = paste0(year(Date), "-", sprintf("%02d", isoweek(Date)))
  )

# Calculate average moon phase by week_num
average_moon_phase_by_week <- moon_phases_df %>%
  group_by(week_num) %>%
  summarise(AverageMoonPhase = mean(MoonPhase, na.rm = TRUE))

# View the results
head(average_moon_phase_by_week)

```

